```{r}
library("rgdal")
library("raster")
library("sf")
library("leafsync")
library("dplyr")
library("ggplot2")
library("stars")
library(spatialEco)
library(spatstat.random)
library(data.table)
library(geoR)
library(RColorBrewer)
library(spdep)
library(spatialreg)
library(classInt)
library(raster)
library(rgeos)
library(landscapemetrics)
library(nimble)
library(coda)
```


```{r load data}
# read in data
cwd.dat<-read.table(
  "./data/wi-dat.csv",
  sep=",", header=TRUE)
wiShp<-st_read("./data/shapefiles/PLSS_Townships/PLSS_Townships.shp")
wiCountyShp <- st_read("./data/shapefiles/County_Boundaries_24K/County_Boundaries_24K.shp")
plot(wiCountyShp)

#subset county data by the study area
wiCountyStudy<-subset(wiCountyShp, COUNTY_FIP %in% unique(cwd.dat$fipsCode))


#lc<-raster("./data/raster/WI_NLCD_2011/WI_NLCD_2011/nlcd_wi_utm16.tif")
## now we can use mask
wiCountyStudy<-st_transform(wiCountyStudy, crs(wiShp))
studyarea<-st_crop(wiShp, wiCountyStudy)

# lc.evergreen <- lc.studyarea==42
# plot(lc.evergreen)# lc.evergreen <- lc.studyarea==42
# plot(lc.evergreen)


# let's read in our new datasheet which is the cwd numbers by county
cwd.dat.county<-read.table(
  "./data/cwd-nums-county-by-year.csv",
  sep=",", header=TRUE)

wi.fips <-read.table(
  "./data/wi-fips.csv",
  sep=",", header=TRUE)

cwd.dat.county.sep<-read.table(
  "./data/cwd-numbers-by-county-by-year-sep.csv",
  sep=",", header=TRUE)

wi.snow <- read.table(
  "./data/wi-snowfall-ind.csv",
  sep=",", header=TRUE)
# wi.snow<-wi.snow[-73,]

#wi.snow<-wi.snow[-1,]

# counties Iowa, Sauk, Dane, Richland
# neighboring counties
# Columbia, Dodge, Jefferson, Rock, Green, Lafayette, Grant, Crawford, # Vernon, Monroe Juneau, Adams

# let's subset this
k<-1

# note that we are interested in the four counties which comprise the highest number
# of total cases Iowa Sauk, Dane, Richland.
# The remaining counties are their direct neighbors
cwd.dat.county.sep.sub <- cwd.dat.county.sep[cwd.dat.county.sep$county %in% c("Iowa", "Sauk", "Dane", "Richland", "Columbia", "Dodge", "Jefferson", "Rock", "Green", "Lafayette", "Grant", "Crawford", "Vernon", "Monroe", "Juneau", "Adams"),]


# cwd.dat.county.rate <- data.frame()
# let's look at rates of increase year over year
# k<-1
# for( i in 1:nrow(cwd.dat.county.sep.sub)){
#   cwd.dat.county.rate[k,] <- cwd.dat.county.sep.sub
# }

# for(i in 1:nrow(cwd.dat.county.sep.sub)){
#   if(cwd.dat.county.sep.sub[i,]$county)
# }

# cwd.rate.of.change<-read.table(
#   "./data/cwd-rate-of-change.csv",
#   sep=",", header=TRUE
# )

# cwd.rate.of.change<-as.data.frame(cwd.rate.of.change)
# cwd.rate.of.change.sub<-cwd.rate.of.change[cwd.rate.of.change$county %in% c("Iowa", "Sauk", "Dane", "Richland"),]

# for(i in 1:nrow(cwd.rate.of.change)){
#   if(cwd.rate.of.change[i,]$X == -100){
#     cwd.rate.of.change[i,]$X <- 0
#   }
# }

wi.cwd.area <- c("Iowa", "Sauk", "Dane", "Richland", "Columbia", "Dodge", "Jefferson", "Rock", "Green", "Lafayette", "Grant", "Crawford", "Vernon", "Monroe", "Juneau", "Adams")

wi.snow.sub <- wi.snow[wi.snow$county %in% c("Iowa", "Sauk", "Dane", "Richland", "Columbia", "Dodge", "Jefferson", "Rock", "Green", "Lafayette", "Grant", "Crawford", "Vernon", "Monroe", "Juneau", "Adams"),]

ggplot(cwd.dat.county.sep.sub, aes(x=cwd.dat.county.sep.sub$year, y=cwd.dat.county.sep.sub$npos, color=cwd.dat.county.sep.sub$county)) +
  geom_line() +
  scale_x_continuous(breaks=round(seq(min(wi.snow.sub$year), max(wi.snow.sub$year), by=1), 1))+
  theme(axis.text.x=element_text(angle=90, hjust=1))

ggplot(wi.snow.sub, aes(x=wi.snow.sub$year, y=log(wi.snow.sub$snowt), color=wi.snow.sub$county)) +
  geom_line() +
  scale_x_continuous(breaks=round(seq(min(wi.snow.sub$year), max(wi.snow.sub$year), by=1), 1))+
  theme(axis.text.x=element_text(angle=90, hjust=1))

ggplot(cwd.rate.of.change.sub, aes(x=cwd.rate.of.change.sub$year, y=cwd.rate.of.change.sub$X, color=cwd.rate.of.change.sub$county)) +
  geom_line() +
  scale_x_continuous(breaks=round(seq(min(cwd.rate.of.change.sub$year), max(cwd.rate.of.change.sub$year), by=1), 1))+
  theme(axis.text.x=element_text(angle=90, hjust=1))



```

```{r clean data}
# remove NA values
cwd.dat<-na.omit(cwd.dat)
yrs<-c(2002:2022)
n.yrs <- length(yrs)
# create UID 
# FIPS - TWNSHP - RANGEID - RANGE DIR
cwd.dat$uid <- 0

for(i in 1:nrow(cwd.dat)){
  cwd.dat[i,]$uid <- paste(cwd.dat[i,]$fipsCode,cwd.dat[i,]$township,cwd.dat[i,]$range, cwd.dat[i,]$rangeDir, sep="-")
}

cwd.mat<-matrix(0, nrow=length(unique(cwd.dat$uid)), ncol=n.yrs)
cwd.mat.samp <- matrix(0,nrow=length(unique(cwd.dat$uid)), ncol=n.yrs)
sites<-unique(cwd.dat$uid)



# aggregate over site
# TODO write to a new sheet in our directory then push to git
for(i in 1:nrow(cwd.dat)){
  for(j in 1:length(sites))
    for(k in 1:length(yrs)){
      if(cwd.dat[i,]$uid == sites[j] &
         cwd.dat[i,]$year == yrs[k])
        cwd.mat[j,k] = cwd.dat[i,]$cwdPositive
        cwd.mat.samp[j,k] = cwd.dat[i,]$numSamp 
      }
}


```

```{r, snowfall data}

# CREATE MATRICES FOR SNOW TOTALS 
# the following are averaged snow-totals for each county in the following winter seasons
# 2002 - 2003, 2003 - 2004, 2004 - 2005, 2005 - 2006, 2006 - 2007, 2007 - 2008,
# 2008 - 2009, 2009 - 2010, 2010 - 2011, 2011 - 2012, 2012 - 2013, 2013 - 2014,
# 2014 - 2015,2014 - 2015, 2016 - 2017, 2017 - 2018, 2018 - 2019, 2019 - 2020,
# 2020 - 2021 2021 - 2022
Dane <- c(43,	32,	49.9,	24.2,	31.1,	27.1,	42.8,	47.7,	59.4,	99.9,	68.7,	40.8,
          63.1,33.3,	63.8,	41.8,	44.5,	32,	36.8,	53.2,	49.5,	50.4,	33.9)
Columbia <- c(47,27.6,53.9,37.2,24.9,31,41.2,49.6,56.9,145.3,93.2,55,74,24.8,
              97.8,55.5,41.5,37.1,47,62,54,41.8,34.3)
Dodge <- c(48,41.9,57.9,29.7,30,41.2,55.4,47.4,51.1,116.7,80.5,52.6,101.4,46.3,
           84.1,54.6,35.5,37.3,57,68,55.5,55.5,47)
Jefferson <- c(42,41.9,49,31.5,28.4,22.6,45.7,44.1,47,110,70,45.1,63.9,41.8,
               75.7,45.1,49.9,42,66.1,63.4,64.1,60.7,42.6)
# cwd.dat <- cwd.dat%>%subset(year >= 2013)
cwd.dat$wsi <- 0

cwd.dat$snow.total <- 0
# counties <-c("dane","dodge", "columbia", "jefferson")
cwd.p <- 0
# for(i in 1:nrow(cwd.dat)){
#   for( j in 1:length(year.samp)){
#         if(cwd.dat[i,]$year == year.samp[j]){
#           if(cwd.dat[i,]$county == "Dane"){
#             cwd.dat[i,]$snow.total <- snow.total[j,]$Dane
#           }
#           if(cwd.dat[i,]$county == "Dodge"){
#             cwd.dat[i,]$snow.total <- snow.total[j,]$Dodge
#           }
#           if(cwd.dat[i,]$county == "Jefferson"){
#             cwd.dat[i,]$snow.total <- snow.total[j,]$Jefferson
#           }
#           if(cwd.dat[i,]$county == "Columbia"){
#             cwd.dat[i,]$snow.total <- snow.total[j,]$Columbia
#           }
#         }
#   }
# }


# make our snow totals matrix
snow.years <- c(1998:2021)
# we'll say for now we'll just subtract the length of the snow years
l.snow.years <- length(snow.years)-1

# let's rbind these into a matrix
snow.totals.four.county.subsample<-as.matrix(rbind(Columbia, Dane, Dodge, Jefferson))
snow.mat <- matrix(0, nrow=length(sites), ncol=ncol(snow.totals.four.county.subsample))

for(i in 1:length(sites)){
  for(j in 1:length(Columbia)){
    if(strsplit(sites[i], "-")[[1]][1] == 21){
      snow.mat[i,j] = Columbia[j]
    }
    if(strsplit(sites[i], "-")[[1]][1] == 25){
      snow.mat[i,j] = Dane[j]
    }
    if(strsplit(sites[i], "-")[[1]][1] == 27){
      snow.mat[i,j] = Dodge[j]
    }
    if(strsplit(sites[i], "-")[[1]][1] == 55){
      snow.mat[i,j] = Jefferson[j]
    }
    
  }
}

# now we can make a big matrix with the repeated values for the uid

for(i in 1:nrow(cwd.dat)){
  for(j in 1:length(sites))
    for(k in 1:length(yrs)){
      if(cwd.dat[i,]$uid == sites[j] &
         cwd.dat[i,]$year == yrs[k])
        cwd.mat[j,k] = cwd.dat[i,]$cwdPositive
        cwd.mat.samp[j,k] = cwd.dat[i,]$numSamp 
      }
}


# 
# for(i in 1:nrow(cwd.dat)){
#   for(j in 1:length(sites)){
#     for(k in 1:ncol(snow.mat)){
#       if(cwd.dat[i,]$uid == sites[j] &
#          cwd.dat[i,]$year)
#     }   
#   }
# }
```

```{r}

# now we want to subset our shapefiles
wiCountyStudy<-subset(wiCountyShp, COUNTY_FIP %in% unique(cwd.dat$fipsCode))

# now we want to subset our county data by the study area
# next we're going to turn this into an extent
# let's first make sure we're working with the same CRS
wiCountyStudy<-st_transform(wiCountyStudy, crs(wiShp))
studyarea<-st_crop(wiShp, wiCountyStudy)
plot(studyarea)
wi.coords<-coordinates(as(studyarea, "Spatial"))

studyarea$cwd.pos <- 0
studyarea$county <- 0
studyarea$year <- 0

for(i in 1:nrow(studyarea)){
  for(j in 1:nrow(cwd.dat)){
    if(studyarea[i,]$TWP == cwd.dat[j,]$township &
       studyarea[i,]$RNG == cwd.dat[j,]$range){
          studyarea[i,]$cwd.pos = studyarea[i,]$cwd.pos + cwd.dat[j,]$cwdPositive
          studyarea[i,]$county = cwd.dat[j,]$county
        }
      }
}

# next we want to attach our deer data to the subset
# so let's create a few variables in our spatial dataframe
deer.knn <- knearneigh(wi.coords)
deer.knn2nb = knn2nb(deer.knn)
deer.list = nbdists(deer.knn2nb, wi.coords)
deer.dist.vec <- unlist(deer.list)
upper.bound.75<-0.50*max(deer.dist.vec)
deer.dnn.nb.75<-dnearneigh(wi.coords, d1=0, d2=upper.bound.75)
deer.dnn.listw.75 <- nb2listw(deer.dnn.nb.75, style="B", zero.policy=TRUE)
deer.dnn.car.out.75 <- spautolm(studyarea$cwd.pos~NULL,
                                data=studyarea$geometry, family="CAR", listw=deer.dnn.listw.75, zero.policy = TRUE)
deer.dnn.car.fitted.75 = fitted(deer.dnn.car.out.75)

studyarea.rook.nb<-poly2nb(studyarea, queen="TRUE")
studyarea.rook.listw = nb2listw(studyarea.rook.nb, style="W", zero.policy=TRUE)
study.area.moran.out = moran.test(studyarea$cwd.pos, listw=studyarea.rook.listw, zero.policy=TRUE)

studyarea$fitted.car <- deer.dnn.car.fitted.75
brks = seq(-4,12, 1)
color.pallete = rev(brewer.pal(length(brks),"RdBu"))
# create 75% of max dist breaks
class.fitted.car.75 = classIntervals(var=studyarea$fitted.car, n=length(brks), style="fixed", fixedBreaks=brks, dataPrecision=1)
color.code.fitted.car.75 = findColours(class.fitted.car.75, color.pallete)

# we need to extract the max and min coordinates
min.coord<-which.min(wi.coords)
max.coord<-which.max(wi.coords)
ggplot(studyarea$geometry, aes(fill=studyarea$fitted.car, ylim=)) +
  geom_sf() +
  guides(fill=guide_legend(title="cwd positive"), color=guide_legend(show=FALSE)) +
  theme(panel.grid=element_blank(),
        panel.background = element_blank())
summary(study.area.moran.out
        )

studyarea.rook.car.out = spautolm(cwd.pos~NULL, data=studyarea, family="CAR", listw=studyarea.rook.listw, zero.policy=TRUE)
summary(studyarea.rook.car.out)

# subset our shapes
wiCountyShp <- subset(wiCountyShp, COUNTY_FIP %in% unique(cwd.dat$fipsCode))
wiCountyShp<-spTransform(wiCountyShp, CRS(proj4string(wishp.shp)))
wiTwnshpShp<-intersect(wiCountyShp,wishp.shp)
```

```{r store data and reload}
# save our CWD matrix so we don't have to redo
write.table(cwd.mat, file="cwd")
cwd.m<-read.table("cwd")
# TODO save our subsetted spatial data
```

ANALYSIS

```{r evaluate temporal autocorrelation}
tmp.corr<-list()
# we need to remove the sites with zero observations
# cwd.m <- cwd.m[1:100,]
# nrow(cwd.m)
# sites <- sites[-c(101:150)]
length(sites)
# first we need to get rid of all those elements in our matrix which have zero variance (i.e. zero observations)
k = 1
var.sites.indx <- c()
mods <- list(type=any)
cwd.m <- na.omit(cwd.m)
# this returns a list of indicies with nonzero variance
for(i in 1:nrow(cwd.m)){
  for(j in 1:ncol(cwd.m)){
    if(cwd.m[i,j] != 0){
      var.sites.indx[k]<-i
      k = k + 1
    }
  }
}
# now we need them to be unique
var.sites.indx <- unique(var.sites.indx)
# we subset our samples this way
cwd.m <- cwd.m[c(var.sites.indx),]
sites <- sites[var.sites.indx]

# ar is called to get estimates for temporal autocorrelation params
for(i in 1:nrow(cwd.m)){
  mods[[i]] <- ar(as.numeric(cwd.m[i,]))
}
ars<-c()
iter <- 1

# we can repeat this for any output of our autocorrelation function that we'd like to see
for(i in 1:length(mods)){
  if("ar" %in% names(mods[[i]]) & length(mods[[i]]$ar) > 0 ){
    for(j in 1:length(mods[[i]]$ar)){
    ars[iter] <- mods[[i]]$ar[j]      
    iter <- iter + 1
    }
  }
}

hist(ars)
```

# for our temporal variance component we can pull the variance between sites
# this is the intra site variance for a single plot
```{r, getting intrasite temporal variance in CWD numbers}
# 
# # note we can also get the between neighbor variance
# site.var <- c()
# for(i in 1:nrow(cwd.mat)){
#   site.var[i]<-var(cwd.mat[i,]) # this will get us the variance for each site (township)
# }
# nonzero.sites.indcs <- c()
# iter = 1
# for(i in 1:length(site.var)){
#   if(site.var[i] != 0){
#     nonzero.sites.indcs[iter] = i
#     iter = iter + 1
#   }
# }
# nonzero.sites.ids <-
# for(i in 1:length())
# plot(site.var)
```


```{r}
n.evergreen <- rep(0, nrow(wiTwnshpShp))
evergreen.clumps <- rep(0, nrow(wiTwnshpShp))
cwd.pos <- rep(0, nrow(wiTwnshpShp))
for(i in 1:nrow(wiTwnshpShp)){
 twnshp<-crop(lc.evergreen, wiTwnshpShp[i,])
 clump<-lsm_c_clumpy(twnshp)
   evergreen.clumps[i] <- if(!is.null(clump[2,]$value)) clump[2,]$value else 2
 for(j in 1:nrow(twnshp)){
   n.evergreen[i] = n.evergreen[i] + sum(twnshp[j,])
   }
}
wiTwnshpShp$n.evergrn <- n.evergreen
wiTwnshpShp$cwd.pos <- 0
wiTwnshpShp$evergreen.clump <- evergreen.clumps
cwd.dat <- na.omit(cwd.dat)
for(i in 1:nrow(wiTwnshpShp)){
  for(j in 1:nrow(cwd.dat)){
    if(wiTwnshpShp[i,]$TWP == cwd.dat[j,]$township &
       wiTwnshpShp[i,]$RNG == cwd.dat[j,]$range){
          cwd.pos[i] = cwd.pos[i] + cwd.dat[j,]$cwdPositive
        }
      }
}
wiTwnshpShp$cwd.pos <- cwd.pos
wiTwnshpShp.subset<-subset(wiTwnshpShp, cwd.pos > 0)
plot(wiTwnshpShp.subset$cwd.pos~wiTwnshpShp.subset$evergreen.clump)
wisf<-st_as_sf(wiTwnshpShp)
ggplot(wisf$geometry, aes(fill=wisf$evergreen.clump)) + 
  geom_sf()
```


```{r, CAR model}
wi.coords <- coordinates(as(studyarea, "Spatial"))
deer.knn <- knearneigh(wi.coords)
deer.knn2nb = knn2nb(deer.knn)
deer.list = nbdists(deer.knn2nb, wi.coords)
deer.dist.vec <- unlist(deer.list)
upper.bound.75<-0.50*max(deer.dist.vec)
deer.dnn.nb.75<-dnearneigh(wi.coords, d1=0, d2=upper.bound.75)
deer.dnn.listw.75 <- nb2listw(deer.dnn.nb.75, style="B", zero.policy=TRUE)
deer.dnn.car.out.75 <- spautolm(studyarea$cwd.pos~NULL,
                                data=studyarea$geometry, family="CAR", listw=deer.dnn.listw.75, zero.policy = TRUE)
deer.dnn.car.fitted.75 = fitted(deer.dnn.car.out.75)

studyarea.rook.nb<-poly2nb(studyarea, queen="TRUE")
studyarea.rook.listw = nb2listw(studyarea.rook.nb, style="W", zero.policy=TRUE)
study.area.moran.out = moran.test(studyarea$cwd.pos, listw=studyarea.rook.listw, zero.policy=TRUE)

studyarea$fitted.car <- deer.dnn.car.fitted.75
brks = seq(-4,12, 1)
color.pallete = rev(brewer.pal(length(brks),"RdBu"))
# create 75% of max dist breaks
class.fitted.car.75 = classIntervals(var=studyarea$fitted.car, n=length(brks), style="fixed", fixedBreaks=brks, dataPrecision=1)
color.code.fitted.car.75 = findColours(class.fitted.car.75, color.pallete)

# we need to extract the max and min coordinates
min.coord<-which.min(wi.coords)
max.coord<-which.max(wi.coords)
ggplot(studyarea$geometry, aes(fill=studyarea$fitted.car, ylim=)) +
  geom_sf() +
  guides(fill=guide_legend(title="cwd positive"), color=guide_legend(show=FALSE)) +
  theme(panel.grid=element_blank(),
        panel.background = element_blank())
summary(study.area.moran.out
        )

studyarea.rook.car.out = spautolm(cwd.pos~NULL, data=studyarea, family="CAR", listw=studyarea.rook.listw, zero.policy=TRUE)
summary(studyarea.rook.car.out)
```

```{r, create neighborhood matrix}

W.nb <- poly2nb(wiTwnshpShp)
W <- nb2WB(W.nb)
```

```{r, nimble spatial model}
# 
# # let's average over the sampling period to get the expected number of positive cases
# # for each PLSS township
# cwd.expected <- rep(0, nrow(wiTwnshpShp))
# cwd.expected <- cwd.pos / length(year.samp)
# 
# wiTwnshpShp$cwd.expected <- cwd.expected
# 
# ## Specify the hierarchical model
# # 
# # dZIP <- nimbleFunction(
# #  run = function(x = integer(), lambda = double(), 
# #                 zeroProb = double(), log = logical(0, default = 0)) {
# #    returnType(double())
# #    ## First handle non-zero data
# #    if (x != 0) {
# #        ## return the log probability if log = TRUE
# #        if (log) return(dpois(x, lambda, log = TRUE) + log(1 - zeroProb))
# #        ## or the probability if log = FALSE
# #       else return((1 - zeroProb) * dpois(x, lambda, log = FALSE))
# #    }
# #    ## From here down we know x is 0
# #    totalProbZero <- zeroProb + (1 - zeroProb) * dpois(0, lambda, log = FALSE)
# #    if (log) return(log(totalProbZero))
# #    return(totalProbZero)
# #  })
# # 
# # registerDistributions(list(
# #     dZIP = list(
# #         BUGSdist = "dZIP(lambda, zeroProb)",
# #         discrete = TRUE,
# #         range = c(0, Inf),
# #         types = c('value = integer()', 'lambda = double()', 'zeroProb = double()')
# #      )))
# 
# # TODO: AVERAGE over years for WSI
# dZIP <- nimbleFunction(
#  run = function(x = integer(), lambda = double(),
#                 zeroProb = double(), log = logical(0, default = 0)) {
#    returnType(double())
#    ## First handle non-zero data
#    if (x != 0) {
#        ## return the log probability if log = TRUE
#        if (log) return(dpois(x, lambda, log = TRUE) + log(1 - zeroProb))
#        ## or the probability if log = FALSE
#       else return((1 - zeroProb) * dpois(x, lambda, log = FALSE))
#    }
#    ## From here down we know x is 0
#    totalProbZero <- zeroProb + (1 - zeroProb) * dpois(0, lambda, log = FALSE)
#    if (log) return(log(totalProbZero))
#    return(totalProbZero)
#  })
# 
# registerDistributions(list(
#     dZIP = list(
#         BUGSdist = "dZIP(lambda, zeroProb)",
#         discrete = TRUE,
#         range = c(0, Inf),
#         types = c('value = integer()', 'lambda = double()', 'zeroProb = double()')
#      )))
# 
# modelcode <- nimbleCode({
#   # likelihood
#   
#   for (i in 1 : N.twnshp) {
#       N[i] ~ dZIP(mu[i], p[i])
#         # our expectation can simply be the average of the given sampling unit over the study period
#        # why not try the average for particular year across all sampling units -- later we can try to make it the average across across a cluster of sampling units 
#         # WSI is our WSI index for a particular county in year i
#         log(mu[i]) <- E[i] + beta0 + beta1*clumps[i] + phi[i] + theta[i]
#         theta[i] ~ dnorm(0.0, prec.h)
#         eta[i] <- theta[i] + phi[i]
#   }
#   
#     # CAR model for spatial random effects
#     phi[1:N.twnshp] ~ dcar_normal(adj[1:L], weights[1:L], num[1:N.twnshp], prec.c, zero_mean=0)
#     
#     ## priors
#     beta0 ~ dnorm(0.0, sd = 100)  # vague prior on intercept
#     beta1 ~ dunif(0,100)  # vague prior on clumpiness covariate effect
#     # beta2 ~ dnorm(0.0, sd = 100) # vague prior on on clumpiness covariate effect 
#     prec.h ~ dgamma(0.001, 0.001)  
#     prec.c ~ dgamma(0.1, 0.1)  
#     
#     # other prior choices for spatial/nonspatial precision
#     # prec.h ~ dgamma(0.32761, 0.181)
#     # prec.c ~ dgamma(0.1, 0.1)
#     
#     # prec.h ~ dgamma(3.2761,1.81)
#     # prec.c ~ dgamma(1.0,1.0)
# 
#     sigma2 <- 1/prec.h
#     tau2 <- 1/prec.c
#     
#     ## calculate alpha
#     sd.h <- sd(theta[1:N.twnshp]) # marginal SD of heterogeneity effects
#     sd.c <- sd(phi[1:N.twnshp])   # marginal SD of clustering (spatial) effects
#     alpha <- sd.c/(sd.h + sd.c)
# })
# 
# p<-runif(nrow(wiTwnshpShp), 0, 1)
# ## Specify data and initial values
# constants <- list(N.twnshp = nrow(wiTwnshpShp), p=p,L = length(W$adj), adj=W$adj, weights=W$weights, num=W$num,
#                   E = wiTwnshpShp$cwd.expected, clumps=wiTwnshpShp$n.evergrn)
# data <- list(N = wiTwnshpShp$cwd.pos)
# inits <- list(beta0 = 0, beta1 = 0, prec.h = 1, prec.c = 1)
# 
# 
# ## Build/Compile model, including steps: 
# ## (1) build model (2) compile model in C++ 
# ## (3) specify MCMC parameters to collect and create MCMC algorithm
# cwdspatmodel <- nimbleModel(modelcode, constants = constants, data = data, inits = inits)
# c.cwdspatmodel <- compileNimble(cwdspatmodel)
# 
# confMC <- configureMCMC(cwdspatmodel, monitors = c('beta0','beta1','sigma2','tau2','eta','alpha'),  enableWAIC = TRUE)
# cwdspatmcmc <- buildMCMC(confMC)
# c.cwdspatmcmc <- compileNimble(cwdspatmcmc, project = cwdspatmodel)
# 
# 
# ## Run MCMC
# mcmc.out <- runMCMC(c.cwdspatmcmc, niter=60000, nburnin=50000, thin=5, nchains=3)
# 
# ## convert post samples as mcmc.list object and diagnose convergence using coda functions
# post.samples <- mcmc.list(sapply(mcmc.out$samples,as.mcmc,simplify=FALSE))
# pars <- c("alpha","beta1","tau2","sigma2","eta[1]")
# plot(post.samples[,pars], trace=TRUE, density=FALSE)
# gelman.plot(post.samples[,pars])
# autocorr.plot(post.samples[,pars])
# 
# ## posterior summary
# summary(post.samples[,pars])
# 
# ## model assessment using WAIC value
# mcmc.out$WAIC
```

```{r}

cwd.by.county<-read.table("./data/cwd-nums-county-by-year.csv",
           header = TRUE, sep=",")

cwd.by.county[is.na(cwd.by.county)]<-0

cwd.by.county <- cwd.by.county[cwd.by.county$county %in% wi.cwd.area,]

wi.snow <- read.table("./data/wi-snowfall.csv",
                      header=TRUE, sep=",") 
  
wi.snow<-wi.snow[wi.snow$X %in% c("Iowa", "Sauk", "Dane", "Richland", "Columbia", "Dodge", "Jefferson", "Rock", "Green", "Lafayette", "Grant", "Crawford", "Vernon", "Monroe", "Juneau", "Adams"),]

wi.snow<-wi.snow[,-2]
wi.snow<-wi.snow[,-1]

wi.snow.mat <- matrix(0, nrow=nrow(wi.snow), ncol=ncol(wi.snow))
for(i in 1:nrow(wi.snow)){
  for(j in 1:ncol(wi.snow)){
    wi.snow.mat[i,j] <- as.numeric(wi.snow[i,j])
  }
}
cwd.by.county.mat <- as.matrix(cwd.by.county[,-1])

adj.matrix<-matrix(c(0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,
                     1,0,0,1,1,0,0,0,0,1,0,0,0,0,1,0,
                     
                     0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,1,
                     0,1,0,0,1,0,1,1,1,0,0,0,0,1,1,0,
                     
                     0,1,0,1,0,0,0,0,1,0,0,0,0,0,0,0,
                     0,0,1,0,0,0,0,1,0,0,1,0,1,0,0,0,
                     0,0,0,1,0,0,0,1,0,0,1,0,0,1,0,0,
                     0,0,1,1,0,1,1,0,0,0,1,0,1,0,1,0,
                     0,0,0,1,1,0,0,0,0,0,0,0,0,1,0,0,
                     
                     1,0,1,0,0,0,0,0,0,0,0,0,0,0,1,1,
                     0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,
                     
                     0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,
                     0,0,1,0,0,1,0,1,0,0,0,0,0,0,1,1,
                     
                     0,0,0,1,0,0,1,0,1,0,0,0,0,0,0,0,
                     0,1,0,1,0,0,0,1,0,0,0,0,1,0,0,1,
                     
                     0,0,1,0,0,0,0,0,0,1,0,0,1,0,1,0), nrow=nrow(cwd.by.county.mat), ncol=nrow(cwd.by.county.mat))
```


```{r,temporal snow depth -- county scale }

# so let's start by getting data for each year
# make a matrix for positives and number sampled


year.samp <- c(2002, 2003, 2004,2006,2007, 2008, 2009, 2010, 2011, 2012, 2013,
               2014, 2015, 2016, 2017, 2018, 2019,2020, 2021,2022)


# let's create our expected value matrix



dZIP <- nimbleFunction(
 run = function(x = integer(), lambda = double(),
                zeroProb = double(), log = logical(0, default = 0)) {
   returnType(double())
   ## First handle non-zero data
   if (x != 0) {
       ## return the log probability if log = TRUE
       if (log) return(dpois(x, lambda, log = TRUE) + log(1 - zeroProb))
       ## or the probability if log = FALSE
      else return((1 - zeroProb) * dpois(x, lambda, log = FALSE))
   }
   ## From here down we know x is 0
   totalProbZero <- zeroProb + (1 - zeroProb) * dpois(0, lambda, log = FALSE)
   if (log) return(log(totalProbZero))
   return(totalProbZero)
 })

registerDistributions(list(
    dZIP = list(
        BUGSdist = "dZIP(lambda, zeroProb)",
        discrete = TRUE,
        range = c(0, Inf),
        types = c('value = integer()', 'lambda = double()', 'zeroProb = double()')
     )))

modelcode <- nimbleCode({
  # likelihood
  
  for (i in 1 : N) {
    for(j in 2 : years){  
        y[i, j] ~ dZIP(mu[i,j], p[i, j])
        # our expectation can simply be the average of the given sampling unit over the study period
        # why not try the average for particular year across all sampling units -- later we can try to make it         the average across across a cluster of sampling units 
        # WSI is our WSI index for a particular county in year i
        mu[i,j] <- beta1*(sum(M[,i]*S[,j+1])/sum(M[,i]))^2 + beta0
    }
  }
  
    # CAR model for spatial random effects
    
  #phi[] ~ dcar_normal(adj[1:L], weights[1:L], num[1:N], prec.c, zero_mean=0)
    
    ## priors
    beta0 ~ dnorm(0.0, sd = 100)  # vague prior on grand intercept
    beta1 ~ dnorm(0.0, sd = 1)  # vague prior snow totals for the county from the year prior
    beta2 ~ dnorm(0.0, sd = 1)
})

# change this to be a matrix of size i x j
p <- matrix(0, nrow=nrow(cwd.by.county.mat), ncol(cwd.by.county.mat))

for(i in 1:nrow(cwd.by.county.mat)){
  for(j in 1:ncol(cwd.by.county.mat)){
    p[i,j] <- runif(1)
  }
}

## Specify data and initial values
constants <- list(N = nrow(cwd.by.county.mat), M=adj.matrix, years=ncol(cwd.by.county.mat), S=wi.snow.mat, p=p)
data <- list(y = cwd.by.county.mat)
inits <- list(beta0 = 0, beta2=0, beta1 = 0)
cwdspatmodel <- nimbleModel(modelcode, constants = constants, data = data, inits = inits)
c.cwdspatmodel <- compileNimble(cwdspatmodel)
confMC <- configureMCMC(cwdspatmodel, monitors = c('beta0', 'beta2', 'beta1'),  enableWAIC = TRUE)
cwdspatmcmc <- buildMCMC(confMC)
c.cwdspatmcmc <- compileNimble(cwdspatmcmc, project = cwdspatmodel)
## Run MCMC
mcmc.out <- runMCMC(c.cwdspatmcmc, niter=40000, nburnin=30000, thin=5, nchains=3, WAIC=TRUE)
## convert post samples as mcmc.list object and diagnose convergence using coda functions
post.samples <- mcmc.list(sapply(mcmc.out$samples,as.mcmc,simplify=FALSE))
pars <- c("beta0","beta1", "beta2")
plot(post.samples[,pars], trace=TRUE, density=FALSE)
gelman.plot(post.samples[,pars])
autocorr.plot(post.samples[,pars])
## posterior summary
summary(post.samples[,pars])
## model assessment using WAIC value
mcmc.out$WAIC
```

```{r, clean WSI data}

wi.wsi <- read.table("./data/wsi-values.csv",
                      header=TRUE, sep=",") 
wsi.sub <- wi.wsi[wi.wsi$name %in% c("Iowa", "Sauk", "Dane", "Richland", "Columbia", "Dodge", "Jefferson", "Rock", "Green", "Lafayette", "Grant", "Crawford", "Vernon", "Monroe", "Juneau", "Adams"),]
wsi.sub <- wsi.sub[,-1]
wsi.mat <- as.matrix(wsi.sub)


cwd.nums.by.cnty.by.yr <- read.table("./data/cwd-numbers-by-county-by-year-sep.csv",
                      header=TRUE, sep=",")

cwd.nums.sep.sub <- subset(cwd.nums.by.cnty.by.yr, cwd.nums.by.cnty.by.yr$county %in% wi.cwd.area)
total.cwd.by.year <- data.frame(year=year.samp, nanlyzd=rep(0, length(year.samp)), cwd.pos=rep(0, length(year.samp)))

for(i in 1:nrow(cwd.nums.by.cnty.by.yr)){
  for(j in 1:nrow(total.cwd.by.year)){
    if(cwd.nums.by.cnty.by.yr[i,]$year == total.cwd.by.year[j,]$year){
      total.cwd.by.year[j,]$nanlyzd = total.cwd.by.year[j,]$nanlyzd + cwd.nums.by.cnty.by.yr[i,]$nanalyzed
      total.cwd.by.year[j,]$cwd.pos = total.cwd.by.year[j,]$cwd.pos + cwd.nums.by.cnty.by.yr[i,]$npos
    }
  }
}
total.cwd.by.year$rate <- 0  
for(i in 1:nrow(total.cwd.by.year)){
  total.cwd.by.year[i,]$rate <- total.cwd.by.year[i,]$cwd.pos/total.cwd.by.year[i,]$nanlyzd
}

cwd.rate <- total.cwd.by.year$rate
```

```{r, make county bneighborhood matrix}
# arrange alphabetically
# this will work but let's figure out how to do this with the packages
wiCountyShp.studyarea <- subset(wiCountyShp, wiCountyShp$COUNTY_NAM %in% wi.cwd.area)
# subset to our study area
wiCountyShp.studyarea <- arrange(wiCountyShp.studyarea, wiCountyShp.studyarea$COUNTY_NAM)

W.nb <- poly2nb(wiCountyShp.studyarea)
# convert to a binary neighborhood matrix of 0's and 1's
adj.mat<-nb2mat(W.nb, style="B")
W <- nb2WB(W.nb)

adj.mat<-matrix(0,nrow=length(wiCountyShp.studyarea$COUNTY_NAM),
       ncol=length(wiCountyShp.studyarea$COUNTY_NAM))

for(i in 1:nrow(adj.mat)){
    k = 1
    while(k < W$num){
      adj.mat[i, W$adj] <- 1  
  }
}

```


```{r, temporal wsi}
# We also include numbers from Fieberg et al. 2008


# so let's start by getting data for each year
# make a matrix for positives and number sampled


year.samp <- c(2002, 2003, 2004,2006,2007, 2008, 2009, 2010, 2011, 2012, 2013,2014, 2015, 2016, 2017, 2018, 2019,2020, 2021,2022)
# let's create our expected value matrix
dZIP <- nimbleFunction(
 run = function(x = integer(), lambda = double(),
                zeroProb = double(), log = logical(0, default = 0)) {
   returnType(double())
   ## First handle non-zero data
   if (x != 0) {
       ## return the log probability if log = TRUE
       if (log) return(dpois(x, lambda, log = TRUE) + log(1 - zeroProb))
       ## or the probability if log = FALSE
      else return((1 - zeroProb) * dpois(x, lambda, log = FALSE))
   }
   ## From here down we know x is 0
   totalProbZero <- zeroProb + (1 - zeroProb) * dpois(0, lambda, log = FALSE)
   if (log) return(log(totalProbZero))
   return(totalProbZero)
 })

registerDistributions(list(
    dZIP = list(
        BUGSdist = "dZIP(lambda, zeroProb)",
        discrete = TRUE,
        range = c(0, Inf),
        types = c('value = integer()', 'lambda = double()', 'zeroProb = double()')
     )))

modelcode <- nimbleCode({
    ### priors
    # vague prior on grand intercept
    beta0~ dnorm(0.0, sd = 100)
    # vague prior WSI values for the county from the year prior
    beta1~ dnorm(0.0, sd = 1)
    ### likelihood
  for (i in 1 : N) {
    for(j in 1 : years){  
        y[i, j] ~ dZIP(mu[i,j], logit(p[i, j]))
        mu[i,j] <- beta1*(sum(M[,i]*S[,j+3])/sum(M[,i])) + beta0
    }
  }
  # CAR model for spatial random effects
  #phi[] ~ dcar_normal(adj[1:L], weights[1:L], num[1:N], prec.c, zero_mean=0)
})

# change this to be a matrix of size i x j
p <- matrix(0, nrow=nrow(cwd.by.county.mat), ncol(cwd.by.county.mat))

for(i in 1:nrow(cwd.by.county.mat)){
  for(j in 1:ncol(cwd.by.county.mat)){
    p[i,j] <- runif(1)
  }
}

## Specify data and initial values
constants <- list(N = nrow(cwd.by.county.mat), M=adj.mat, years=ncol(cwd.by.county.mat), S=wsi.mat, p=p)
data <- list(y = cwd.by.county.mat)
inits <- list(beta0 = 0, beta1 = 0)
cwdspatmodel <- nimbleModel(modelcode, constants = constants, data = data, inits = inits)
c.cwdspatmodel <- compileNimble(cwdspatmodel)
confMC <- configureMCMC(cwdspatmodel, monitors = c('beta0', 'beta1'),  enableWAIC = TRUE)
cwdspatmcmc <- buildMCMC(confMC)
c.cwdspatmcmc <- compileNimble(cwdspatmcmc, project = cwdspatmodel)
## Run MCMC
mcmc.out <- runMCMC(c.cwdspatmcmc, niter=40000, nburnin=30000, thin=5, nchains=3, WAIC=TRUE)
## convert post samples as mcmc.list object and diagnose convergence using coda functions
post.samples <- mcmc.list(sapply(mcmc.out$samples,as.mcmc,simplify=FALSE))
pars <- c("beta0","beta1")
plot(post.samples, trace=TRUE, density=FALSE)
gelman.plot(post.samples)
autocorr.plot(post.samples)
## posterior summary
summary(post.samples)
## model assessment using WAIC value
mcmc.out$WAIC
```

```{r, temporal wsi add var}

# so let's start by getting data for each year
# make a matrix for positives and number sampled


year.samp <- c(2002, 2003, 2004,2006,2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019,2020, 2021,2022)
# let's create our expected value matrix
dZIP <- nimbleFunction(
 run = function(x = integer(), lambda = double(),
                zeroProb = double(), log = logical(0, default = 0)) {
   returnType(double())
   ## First handle non-zero data
   if (x != 0) {
       ## return the log probability if log = TRUE
       if (log) return(dpois(x, lambda, log = TRUE) + log(1 - zeroProb))
       ## or the probability if log = FALSE
      else return((1 - zeroProb) * dpois(x, lambda, log = FALSE))
   }
   ## From here down we know x is 0
   totalProbZero <- zeroProb + (1 - zeroProb) * dpois(0, lambda, log = FALSE)
   if (log) return(log(totalProbZero))
   return(totalProbZero)
 })

registerDistributions(list(
    dZIP = list(
        BUGSdist = "dZIP(lambda, zeroProb)",
        discrete = TRUE,
        range = c(0, Inf),
        types = c('value = integer()', 'lambda = double()', 'zeroProb = double()')
     )))

modelcode <- nimbleCode({
    ### priors
    # vague prior on grand intercept
    beta0~ dnorm(0.0, sd = 100)
    # vague prior WSI values for the county from the year prior
    beta1~ dnorm(0.0, sd = 1)
    ### likelihood
  for (i in 1 : N) {
    for(j in 1 : years){  
        mu[i,j] <- beta1*(sum(M[,i]*S[,j+3])/sum(M[,i])) + beta0
        pi ~ dbern(p[i])
        y[i, j] ~ dZIP(mu[i,j], pi)
        }
  }
  # CAR model for spatial random effects
  #phi[] ~ dcar_normal(adj[1:L], weights[1:L], num[1:N], prec.c, zero_mean=0)
})

# change this to be a matrix of size i x j
# p <- matrix(0, nrow=nrow(cwd.by.county.mat), ncol(cwd.by.county.mat))
# 
# for(i in 1:nrow(cwd.by.county.mat)){
#   for(j in 1:ncol(cwd.by.county.mat)){
#     p[i,j] <- bern
#   }
# }

## Specify data and initial values
constants <- list(N = nrow(cwd.by.county.mat), M=adj.matrix, years=ncol(cwd.by.county.mat), S=wsi.mat, p=cwd.rate)
data <- list(y = cwd.by.county.mat)
inits <- list(beta0 = 0, beta1 = 0)
cwdspatmodel <- nimbleModel(modelcode, constants = constants, data = data, inits = inits)
c.cwdspatmodel <- compileNimble(cwdspatmodel)
confMC <- configureMCMC(cwdspatmodel, monitors = c('beta0', 'beta1'),  enableWAIC = TRUE)
cwdspatmcmc <- buildMCMC(confMC)
c.cwdspatmcmc <- compileNimble(cwdspatmcmc, project = cwdspatmodel)
## Run MCMC
mcmc.out <- runMCMC(c.cwdspatmcmc, niter=40000, nburnin=30000, thin=5, nchains=3, WAIC=TRUE)
## convert post samples as mcmc.list object and diagnose convergence using coda functions
post.samples <- mcmc.list(sapply(mcmc.out$samples,as.mcmc,simplify=FALSE))
pars <- c("beta0","beta1")
plot(post.samples, trace=TRUE, density=FALSE)
gelman.plot(post.samples)
autocorr.plot(post.samples)
## posterior summary
summary(post.samples)
## model assessment using WAIC value
mcmc.out$WAIC
```