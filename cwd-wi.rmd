```{r}
library("rgdal")
library("raster")
library("sf")
library("leafsync")
library("dplyr")
library("ggplot2")
library("stars")
library(spatialEco)
library(spatstat.random)
library(data.table)
library(geoR)
library(RColorBrewer)
library(spdep)
library(spatialreg)
library(classInt)
library(rgeos)
library(landscapemetrics)
library(nimble)
library(coda)
library(prism)
library(terra)
```


```{r load data}
# read in data
cwd.dat<-read.table(
  "./data/wi-dat.csv",
  sep=",", header=TRUE)
wi.small.subset <- c("Dane", "Columbia", "Dodge", "Jefferson", "Calumet", "Chippewa", "Crawford", "Eau Claire", "Iowa", "Richland", "Adams", "Walworth")
wiShp<-st_read("./data/shapefiles/PLSS_Townships/PLSS_Townships.shp")
wiCountyShp <- st_read("./data/shapefiles/County_Boundaries_24K/County_Boundaries_24K.shp")
wiTwnshpShp <- st_read("./data/shapefiles/PLSS_Townships/PLSS_Townships.shp")
#plot(wiCountyShp)

#subset county data by the study area
wiCountyStudy<-subset(wiCountyShp, COUNTY_NAM %in% wi.small.subset)
lc<-raster("./data/raster/WI_NLCD_2011/WI_NLCD_2011/nlcd_wi_utm16.tif")
## now we can use mask
wiCountyStudy<-st_transform(wiCountyStudy, crs(wiShp))
studyarea<-st_intersection(wiShp, wiCountyStudy)



# let's read in our new datasheet which is the cwd numbers by county
cwd.dat.county<-read.table(
  "./data/cwd-nums-county-by-year.csv",
  sep=",", header=TRUE)

wi.fips <-read.table(
  "./data/wi-fips.csv",
  sep=",", header=TRUE)

cwd.dat.county.sep<-read.table(
  "./data/cwd-numbers-by-county-by-year-sep.csv",
  sep=",", header=TRUE)

wiTwnshpShp <- st_read("./data/shapefiles/PLSS_Townships/PLSS_Townships.shp")

cwd.dat.pos<-read.table(
  "./data/wi-dat-cwd-pos.csv",
  sep=",", header=TRUE)
cwd.dat.analyzed<-read.table(
  "./data/cwd-dat-num-analyzed.csv",
  sep=",", header=TRUE)
wi.wsi <- read.table("./data/WSI-MAST-CORRECTED.csv",
                      header=TRUE, sep=",") 
k<-1
studyarea <- subset(studyarea, studyarea$COUNTY_NAM %in% wi.small.subset)

# grab our prism data
# first we need to set our path for where we want prism to store the data
prism_set_dl_dir('C:\\Users\\jackx\\Desktop\\prism-dat-f') # only set to create =TRUE if we need to download again
mnths <- c(11, 12, 1, 2, 3, 4, 5)
yrs<-c(seq(1999, 2022, 1))
get_prism_monthlys(type="ppt",
                 mon=mnths,
                 years=yrs,
                 keepZip = TRUE)

get_prism_dailys(type="ppt",
                 minDate="2012-11-01",
                 maxDate="2022-05-31",
                 keepZip = TRUE)
# note that this didn't finish tmax
# get_prism_dailys(type="tmax",
#                  minDate="1999-11-01",
#                  maxDate="2022-05-31",
#                  keepZip=TRUE)

pdStackRaw<-pd_stack(prism_archive_subset("ppt",
                        "monthly",
                        mon=c(12, 1, 2, 3, 4),
                        years=seq(1999, 2022, 1)))
# pdPoly<-rasterToPolygons(pdStackRaw)

pdStackR <- pdStackRaw
```
```{r clean data}
# remove NA values
# cwd.dat<-na.omit(cwd.dat)
yrs<-c(2002:2022)
n.yrs <- length(yrs)

# let's work with the prism data
# first we'll restrict the extent to approximately WI
# 
#     "name": "Wisconsin",
#     "min_lat": 42.4954,
#     "max_lat": 47.31,
#     "min_lng": -92.8564,
#     "max_lng": -86.2523
#   },
pdStackDF<-data.frame(rasterToPoints(pdStackR))



for(i in 1:nrow(cwd.dat.pos)){
  for(j in 1:ncol(cwd.dat.pos)){
    if(is.na(cwd.dat.pos[i,j])){
      cwd.dat.pos[i,j] <- 0
    }
  }
}

for(i in 1:nrow(cwd.dat.analyzed)){
  for(j in 1:ncol(cwd.dat.analyzed)){
    if(is.na(cwd.dat.analyzed[i,j])){
      cwd.dat.analyzed[i,j] <- 0
    }
  }
}
cwd.dat.analyzed$fid <- 0
cwd.dat.pos$fid <- 0
for(i in 1:nrow(cwd.dat.pos)){
  for(j in 1:nrow(studyarea)){
    if(paste0(studyarea[j,]$COUNTY_FIP, "-",
                             studyarea[j,]$TWP, "-",
                             studyarea[j,]$RNG, "-",
                             studyarea[j,]$DIR_ALPHA) == cwd.dat.pos[i,]$uid){
       cwd.dat.pos[i,]$fid <- studyarea[j,]$FID
        cwd.dat.analyzed[i,]$fid <- studyarea[j,]$FID 
    }
  }
}

studyarea$uid <- 0
for(j in 1:nrow(studyarea)){
    studyarea[j,]$uid <- paste0(studyarea[j,]$COUNTY_FIP, "-",
                             studyarea[j,]$TWP, "-",
                             studyarea[j,]$RNG, "-",
                             studyarea[j,]$DIR_ALPHA)

    }

studyarea <- studyarea[-c(which(!c(studyarea$uid) %in% c(cwd.dat.pos$uid))),]
# # studyarea <- studyarea[-c(duplicated(studyarea$FID)),] # get rid of duplicates
# cwd.dat.pos <- cwd.dat.pos[-c(duplicated(cwd.dat.pos$fid)),]
# cwd.dat.analyzed <- cwd.dat.analyzed[-c(duplicated(cwd.dat.analyzed$fid))),]
cwd.dat.pos<-cwd.dat.pos[-c(which(cwd.dat.pos$fid == 0)),] # get rid of those without matching spatial fid
cwd.dat.analyzed<-cwd.dat.analyzed[-c(which(cwd.dat.analyzed$fid==0)),]
studyarea<-arrange(studyarea, studyarea$uid)
cwd.dat.pos<-arrange(cwd.dat.pos, cwd.dat.pos$uid)
cwd.dat.analyzed<-arrange(cwd.dat.analyzed, cwd.dat.analyzed$uid)

cwd.mat<-matrix(0, nrow=length(unique(cwd.dat.pos$fid)), ncol=n.yrs)
cwd.mat.anlyzd <- cwd.mat

cwd.mat <- as.matrix(cwd.dat.pos[,-1])
cwd.mat <- cwd.mat[,-ncol(cwd.mat)]
cwd.mat.anlyzd <- as.matrix(cwd.dat.analyzed[,-1])
cwd.mat.anlyzd <- cwd.mat.anlyzd[,-ncol(cwd.mat.anlyzd)]

for(i in 1:nrow(cwd.mat)){
  for(j in 1:ncol(cwd.mat)){
    if(is.na(cwd.mat[i,j])){
      cwd.mat[i,j] = 0
    }
  }
}
```

```{r, calculate landscape stats twnshp}
studyarea.spat<-as(studyarea, "Spatial")
studyarea.spat<-spTransform(studyarea.spat, crs(lc))
lc<-crop(lc, studyarea.spat)

# studyarea<-st_as_sf(studyarea) # convert back to sf object
prop.evrgrn <- c() 
lc.evrgrn <- lc == 42
# studyarea<-as(studyarea, "Spatial")
for(i in 1:nrow(studyarea.spat)){
  evrgrn <-crop(lc.evrgrn,extent(bbox(studyarea.spat[i,])))
  prop.evrgrn[i] <- cellStats(evrgrn, "sum") / ncell(evrgrn)
}

```






```{r}
# now we want to subset our shapefiles
wiCountyStudy<-subset(wiCountyShp, COUNTY_FIP %in% unique(cwd.dat$fipsCode))
# now we want to subset our county data by the study area
# next we're going to turn this into an extent
# let's first make sure we're working with the same CRS
wiCountyStudy<-st_transform(wiCountyStudy, crs(wiShp))
studyarea<-st_crop(wiShp, wiCountyStudy)
plot(studyarea)

studyarea$cwd.pos <- 0
studyarea$county <- 0

#order(cwd.dat, cwd.dat$county, cwd.dat$)

wi.twnshp.df<-data.frame(unique(cwd.dat$uid),
           y1=c(0), y2=c(0), y3=c(0), y4=c(0), y5=c(0),
           y6=c(0), y7=c(0), y8=c(0), y9=c(0), y10=c(0),
           y11=c(0), y12=c(0), y13=c(0), y14=c(0), y15=c(0),
           y16=c(0), y17=c(0), y18=c(0), y19=c(0), y20=c(0))


for(i in 1:nrow(studyarea)){
  for(j in 1:nrow(cwd.dat)){
    for(k in 1:length(year.samp)){
      if(studyarea[i,]$TWP == cwd.dat[j,]$township &
        studyarea[i,]$RNG == cwd.dat[j,]$range & 
        year.samp[k] == cwd.dat[j,]$year){
        studyarea[i,k]$cwd.pos = studyarea[i,]$cwd.pos + cwd.dat[j,]$cwdPositive
          studyarea[i,]$county = cwd.dat[j,]$county
        }
      }
  }
}

# remove the last three letters
for( i in 1 : nrow(cwd.dat)){
  cwd.dat$uid <- strsplit(cwd.dat$uid, "ast")[[i]]
}

for(i in 1:nrow(studyarea)){
  for(j in 1:nrow(cwd.dat)){
    for(k in 1:length(year.samp)){
      if(!(studyarea[i,]$TWP == cwd.dat[j,]$township &
        studyarea[i,]$RNG == cwd.dat[j,]$range & 
        year.samp[k] == cwd.dat[j,]$year)){
        studyarea[i,k]$cwd.pos = studyarea[i,]$cwd.pos + cwd.dat[j,]$cwdPositive
          studyarea[i,]$county = cwd.dat[j,]$county
        }
      }
  }
}

wiTwnshpShp<-intersect(wiCountyShp,wiShp)
```

```{r store data and reload}
# save our CWD matrix so we don't have to redo
write.table(cwd.mat, file="cwd.pos")
write.table(cwd.mat.anlyzd, file="cwd.analyzed")
cwd.mat.pos<-read.table("cwd.pos")
cwd.mat.analyzed <- read.table("cwd.analyzed")
# TODO save our subsetted spatial data
```

ANALYSIS

```{r, examine residuals}

cwd.r <- c(rep(0,ncol(cwd.mat.pos)))
for(i in 1:ncol(cwd.mat.pos)){
  cwd.r[i] <- sum(cwd.mat.pos[,i])/sum(cwd.mat.analyzed[,i])
}

cwd.exp <- cwd.r * cwd.mat.analyzed
cwd.resid <- (cwd.mat.pos) - (cwd.exp)

wi.wsi.sub<-subset(wi.wsi, wi.wsi$name %in% wi.small.subset)
wi.wsi.sub<-wi.wsi.sub[,-c(1:4)]
# wi.wsi.sub<-wi.wsi.sub[,-c(ncol(wi.wsi.sub))]
#wi.wsi.sub<-wi.wsi.sub[,-ncol(wi.wsi.sub)]
wi.wsi.mat <- matrix(0, ncol=n.yrs, nrow=(nrow(cwd.resid)))
twnshpcount <- c()
twnshps <- unique(studyarea$COUNTY_NAM)
k <- 1
for(i in 1:length(twnshps)){
  twnshpcount[k] <- length(which(studyarea$COUNTY_NAM == twnshps[i]))
  k<-k+1
}
wsi.vec <- c()
l<- 1
for(i in 1:length(twnshpcount)){
    twnshpdat <- rep(wi.wsi.sub[i,], twnshpcount[i])
  for(k in 1:length(twnshpdat)){
    wsi.vec[l] <- twnshpdat[[k]]
    l <- l + 1
    }
}
resid.vec <- c()
prop.evrgrn.exp <- c()
cwd.tot <- c()
k<-1
for(i in 1:nrow(cwd.resid)){
  for(j in 1:ncol(cwd.resid)){
    resid.vec[k] <- cwd.resid[i,j]
    cwd.tot[k] <- cwd.mat[i,j]
    prop.evrgrn.exp[k] <- prop.evrgrn[i]
    k <- k + 1
  }
}
bd.ind <- which(resid.vec==0)
resid.vec <- resid.vec[-c(bd.ind)]
cwd.tot <- cwd.tot[-c(bd.ind)]
wsi.vec <- wsi.vec[-c(bd.ind)]
prop.evrgrn.exp<- prop.evrgrn.exp[-c(bd.ind)]



# #resid.vec<-as.vector(cwd.resid)
resid.vec <- resid.vec + 1
prop.evrgrn.exp <- prop.evrgrn.exp + 1
cwd.tot <- cwd.tot + 1
wsi.vec <- wsi.vec + 1

tot<-cbind(resid.vec, cwd.tot, prop.evrgrn.exp, wsi.vec)
tot.df<-as.data.frame(tot)

tot.df$resid.vec <- log(tot.df$resid.vec)
tot.df$prop.evrgrn.exp <- log(tot.df$prop.evrgrn.exp)
tot.df$wsi.vec  <- log(tot.df$wsi.vec)

tot.df$bin <- cut(tot.df$wsi.vec, c(seq(from = min(tot.df$wsi.vec), to = max(tot.df$wsi.vec), by = 0.2)))

ggplot(tot.df) + geom_boxplot(aes(bin, resid.vec))
  

# plot(log(resid.vec)~ log(prop.evrgrn.exp))
# abline(lm(log(resid.vec) ~ log(prop.evrgrn.exp)), col="red")
# plot(log(resid.vec) ~ log(wsi.vec))
# abline(lm(log(resid.vec) ~ log(wsi.vec)), col="red")
# # we can also look at the totals
# plot(cwd.tot~ prop.evrgrn.exp)
# abline(lm(cwd.tot ~ prop.evrgrn.exp), col="red")
# plot(cwd.tot ~ wsi.vec)
# abline(lm(cwd.tot ~ wsi.vec), col="red")

cwd.exp.vec <- c()
cwd.tot <- c()
k<-1
for(i in 1:nrow(cwd.resid)){
  for(j in 1:ncol(cwd.resid)){
    cwd.exp.vec[k] <- cwd.exp[i,j]
    cwd.tot[k] <- cwd.mat.pos[i,j]
    # prop.evrgrn.exp[k] <- prop.evrgrn[i]
    k <- k + 1
  }
}

exp<-cwd.exp.vec + wsi.vec

# resid.vec <- as.vector(cwd.resid)
#cwd.tot <- as.vector(cwd.mat.pos
plot(log(cwd.exp.vec)~log(cwd.tot))
abline(a=0,b=1, col="red")
```

```{r evaluate temporal autocorrelation}
tmp.corr<-list()
# we need to remove the sites with zero observations
# cwd.m <- cwd.m[1:100,]
# nrow(cwd.m)
# sites <- sites[-c(101:150)]
length(sites)
# first we need to get rid of all those elements in our matrix which have zero variance (i.e. zero observations)
k = 1
var.sites.indx <- c()
mods <- list(type=any)
cwd.m <- na.omit(cwd.m)
# this returns a list of indicies with nonzero variance
for(i in 1:nrow(cwd.m)){
  for(j in 1:ncol(cwd.m)){
    if(cwd.m[i,j] != 0){
      var.sites.indx[k]<-i
      k = k + 1
    }
  }
}
# now we need them to be unique
var.sites.indx <- unique(var.sites.indx)
# we subset our samples this way
cwd.m <- cwd.m[c(var.sites.indx),]
sites <- sites[var.sites.indx]

# ar is called to get estimates for temporal autocorrelation params
for(i in 1:nrow(cwd.m)){
  mods[[i]] <- ar(as.numeric(cwd.m[i,]))
}
ars<-c()
iter <- 1

# we can repeat this for any output of our autocorrelation function that we'd like to see
for(i in 1:length(mods)){
  if("ar" %in% names(mods[[i]]) & length(mods[[i]]$ar) > 0 ){
    for(j in 1:length(mods[[i]]$ar)){
    ars[iter] <- mods[[i]]$ar[j]      
    iter <- iter + 1
    }
  }
}

hist(ars)
```

```{r, CAR model}
wi.coords <- coordinates(as(studyarea, "Spatial"))
deer.knn <- knearneigh(wi.coords)
deer.knn2nb = knn2nb(deer.knn)
deer.list = nbdists(deer.knn2nb, wi.coords)
deer.dist.vec <- unlist(deer.list)
upper.bound.75<-0.50*max(deer.dist.vec)
deer.dnn.nb.75<-dnearneigh(wi.coords, d1=0, d2=upper.bound.75)
deer.dnn.listw.75 <- nb2listw(deer.dnn.nb.75, style="B", zero.policy=TRUE)
deer.dnn.car.out.75 <- spautolm(studyarea$cwd.pos~NULL,
                                data=studyarea$geometry, family="CAR", listw=deer.dnn.listw.75, zero.policy = TRUE)
deer.dnn.car.fitted.75 = fitted(deer.dnn.car.out.75)

studyarea.rook.nb<-poly2nb(studyarea, queen="TRUE")
studyarea.rook.listw = nb2listw(studyarea.rook.nb, style="W", zero.policy=TRUE)
study.area.moran.out = moran.test(studyarea$cwd.pos, listw=studyarea.rook.listw, zero.policy=TRUE)

studyarea$fitted.car <- deer.dnn.car.fitted.75
brks = seq(-4,12, 1)
color.pallete = rev(brewer.pal(length(brks),"RdBu"))
# create 75% of max dist breaks
class.fitted.car.75 = classIntervals(var=studyarea$fitted.car, n=length(brks), style="fixed", fixedBreaks=brks, dataPrecision=1)
color.code.fitted.car.75 = findColours(class.fitted.car.75, color.pallete)

# we need to extract the max and min coordinates
min.coord<-which.min(wi.coords)
max.coord<-which.max(wi.coords)
ggplot(studyarea$geometry, aes(fill=studyarea$fitted.car, ylim=)) +
  geom_sf() +
  guides(fill=guide_legend(title="cwd positive"), color=guide_legend(show=FALSE)) +
  theme(panel.grid=element_blank(),
        panel.background = element_blank())
summary(study.area.moran.out
        )

studyarea.rook.car.out = spautolm(cwd.pos~NULL, data=studyarea, family="CAR", listw=studyarea.rook.listw, zero.policy=TRUE)
summary(studyarea.rook.car.out)
```

```{r, create neighborhood matrix}

W.nb <- poly2nb(wiTwnshpShp)
W <- nb2WB(W.nb)
```

```{r warning=FALSE, nimble spatial model}
# let's create our expected value
cwd.r <- rep(0, ncol(cwd.mat.pos))
for(j in 1:ncol(cwd.mat.pos)){
  cwd.r[j]<-sum(cwd.mat.pos[,j])/sum(cwd.mat.analyzed[,j])  
}


W.nb <- poly2nb(studyarea)
W <- nb2WB(W.nb)
# TODO: AVERAGE over years for WSI
dZIP <- nimbleFunction(
 run = function(x = integer(), lambda = double(),
                zeroProb = double(), log = logical(0, default = 0)) {
   returnType(double())
   ## First handle non-zero data
   if (x != 0) {
       ## return the log probability if log = TRUE
       if (log) return(dpois(x, lambda, log = TRUE) + log(1 - zeroProb))
       ## or the probability if log = FALSE
      else return((1 - zeroProb) * dpois(x, lambda, log = FALSE))
   }
   ## From here down we know x is 0
   totalProbZero <- zeroProb + (1 - zeroProb) * dpois(0, lambda, log = FALSE)
   if (log) return(log(totalProbZero))
   return(totalProbZero)
 })

registerDistributions(list(
    dZIP = list(
        BUGSdist = "dZIP(lambda, zeroProb)",
        discrete = TRUE,
        range = c(0, Inf),
        types = c('value = integer()', 'lambda = double()', 'zeroProb = double()')
     )))

modelcode <- nimbleCode({
  # likelihood

  for (i in 1 : n.twnshps) {
    for(j in 1: n.years){
      
    beta0 ~ dnorm(0.0, sd = 100)  # vague prior on intercept
    gamma0 ~dnorm(0.0, sd=100)
        # our expectation can simply be the average of the given sampling unit over the study period
       # why not try the average for particular year across all sampling units -- later we can try to make it the average across across a cluster of sampling units
        # WSI is our WSI index for a particular county in year i
        log(mu[i,j]) <-  log(s[i,j]*mu.hat[j]) + beta0 
        logit(p[i,j]) <- log(s[i,j]*mu.hat[j]) + gamma0
        y[i,j] ~ dZIP(mu[i,j], p[i,j])
        }
  }

    # CAR model for spatial random effects
    #$phi[1:n.twnshps] ~ dcar_normal(adj[1:L], weights[1:L], num[1:n.twnshps], prec.c, zero_mean=0)
    #psi[1:n.twnshps] ~ dcar_normal(adj[1:L], weights[1:L], num[1:n.twnshps], prec.c, zero_mean=0)
})

p <- matrix(0, nrow=nrow(cwd.mat.anlyzd), ncol=ncol(cwd.mat.anlyzd))
## Specify data and initial values
constants <- list(n.twnshps = nrow(cwd.mat.anlyzd),
                  p=p,
                  mu.hat=cwd.r,
                  n.years=n.yrs)
data <- list(y = cwd.mat.pos, s = cwd.mat.analyzed)
inits <- list(beta0 = 0, gamma0 = 0)


## Build/Compile model, including steps:
## (1) build model (2) compile model in C++
## (3) specify MCMC parameters to collect and create MCMC algorithm
cwdspatmodel <- nimbleModel(modelcode, constants = constants, data = data, inits = inits)
c.cwdspatmodel <- compileNimble(cwdspatmodel)

confMC <- configureMCMC(cwdspatmodel, monitors = c('beta0','gamma0'),  enableWAIC = TRUE)
cwdspatmcmc <- buildMCMC(confMC)
c.cwdspatmcmc <- compileNimble(cwdspatmcmc, project = cwdspatmodel)
## Run MCMC
mcmc.out <- runMCMC(c.cwdspatmcmc, niter=40000, nburnin=30000, thin=5, nchains=3, WAIC=TRUE)
## convert post samples as mcmc.list object and diagnose convergence using coda functions
post.samples <- mcmc.list(sapply(mcmc.out$samples,as.mcmc,simplify=FALSE))
pars <- c("beta0",'gamma0')
plot(post.samples, trace=TRUE, density=FALSE)
gelman.plot(post.samples)
autocorr.plot(post.samples)
## posterior summary
summary(post.samples)
## model assessment using WAIC value
mcmc.out$WAIC

```

```{r warning=FALSE, nimble spatial model}
# let's create our expected value

cwd.r<-sum(cwd.mat.pos)/sum(cwd.mat.analyzed)

W.nb <- poly2nb(studyarea)
W <- nb2WB(W.nb)
# TODO: AVERAGE over years for WSI
dZIP <- nimbleFunction(
 run = function(x = integer(), lambda = double(),
                zeroProb = double(), log = logical(0, default = 0)) {
   returnType(double())
   ## First handle non-zero data
   if (x != 0) {
       ## return the log probability if log = TRUE
       if (log) return(dpois(x, lambda, log = TRUE) + log(1 - zeroProb))
       ## or the probability if log = FALSE
      else return((1 - zeroProb) * dpois(x, lambda, log = FALSE))
   }
   ## From here down we know x is 0
   totalProbZero <- zeroProb + (1 - zeroProb) * dpois(0, lambda, log = FALSE)
   if (log) return(log(totalProbZero))
   return(totalProbZero)
 })

registerDistributions(list(
    dZIP = list(
        BUGSdist = "dZIP(lambda, zeroProb)",
        discrete = TRUE,
        range = c(0, Inf),
        types = c('value = integer()', 'lambda = double()', 'zeroProb = double()')
     )))

modelcode <- nimbleCode({
    ## priors
    beta0 ~ dnorm(0.0, sd = 100)  # vague prior on intercept
    beta1~ dnorm(0.0, sd = 1)
    gamma0 ~ dnorm(0.0, sd=100)  # vague prior on clumpiness covariate effect
    gamma1 ~ dnorm(0.0, sd=1)
    prec.c ~ dgamma(0.1, 0.1)
    prec.ci ~ dgamma(0.1, 0.1)
  
    # CAR model for spatial random effects
    phi[1:n.twnshps] ~ dcar_normal(adj[1:L], weights[1:L], num[1:n.twnshps], prec.c, zero_mean=0)
    psi[1:n.twnshps] ~ dcar_normal(adj[1:L], weights[1:L], num[1:n.twnshps], prec.ci, zero_mean=0)
    
  # likelihood

  for (i in 1 : n.twnshps) {
    for(j in 1: n.years){
        # our expectation can simply be the average of the given sampling unit over the study period
       # why not try the average for particular year across all sampling units -- later we can try to make it the average across across a cluster of sampling units
        # WSI is our WSI index for a particular county in year i
        log(mu[i,j]) <-  log(s[i,j]*mu.hat) + beta0 + beta1*prop.evrgrn[i] +  phi[i]
        logit(p[i,j]) <- log(s[i,j]*mu.hat) + gamma0 + gamma1*prop.evrgrn[i] +   psi[i]
        y[i,j] ~ dZIP(mu[i,j], p[i,j])
        }
  }

    ## calculate alpha
    sd.h <- sd(phi[1:n.twnshps]) # marginal SD of heterogeneity effects
    sd.hi <- sd(psi[1:n.twnshps])   # marginal SD of clustering (spatial) effects
})

p <- matrix(0, nrow=nrow(cwd.mat.anlyzd), ncol=ncol(cwd.mat.anlyzd))
## Specify data and initial values
constants <- list(n.twnshps = nrow(cwd.mat.anlyzd),
                  p=p,
                  mu.hat=cwd.r,
                  L = length(W$adj),
                  adj=W$adj,
                  weights=W$weights,
                  num=W$num,
                  n.years=n.yrs)
data <- list(y = cwd.mat.pos, s = cwd.mat.analyzed, prop.evrgrn=prop.evrgrn)
inits <- list(beta0 = 0, beta1=0, gamma1=0, gamma0 = 0, prec.h = 1, prec.hi = 1)


## Build/Compile model, including steps:
## (1) build model (2) compile model in C++
## (3) specify MCMC parameters to collect and create MCMC algorithm
cwdspatmodel <- nimbleModel(modelcode, constants = constants, data = data, inits = inits)
c.cwdspatmodel <- compileNimble(cwdspatmodel)

confMC <- configureMCMC(cwdspatmodel, monitors = c('beta0', 'beta1', 'gamma1','gamma0', 'sd.h', 'sd.hi'),  enableWAIC = TRUE)
cwdspatmcmc <- buildMCMC(confMC)
c.cwdspatmcmc <- compileNimble(cwdspatmcmc, project = cwdspatmodel)
## Run MCMC
mcmc.out <- runMCMC(c.cwdspatmcmc, niter=40000, nburnin=30000, thin=5, nchains=3, WAIC=TRUE)
## convert post samples as mcmc.list object and diagnose convergence using coda functions
post.samples <- mcmc.list(sapply(mcmc.out$samples,as.mcmc,simplify=FALSE))
pars <- c("beta0",'gamma0', 'beta1', 'gamma1', 'sd.h', 'sd.hi')
plot(post.samples, trace=TRUE, density=FALSE)
gelman.plot(post.samples)
autocorr.plot(post.samples)
## posterior summary
summary(post.samples)
## model assessment using WAIC value
mcmc.out$WAIC
```

```{r}
cwd.by.county.mat <- as.matrix(cwd.by.county[,-1])
year.samp <- c(2002, 2003, 2004,2006,2007, 2008, 2009, 2010, 2011, 2012, 2013,
               2014, 2015, 2016, 2017, 2018, 2019,2020, 2021,2022)
```

```{r, clean WSI data}

wsi.sub <- wi.wsi[wi.wsi$name %in% c("Iowa", "Sauk", "Dane", "Richland", "Columbia", "Dodge", "Jefferson", "Rock", "Green", "Lafayette", "Grant", "Crawford", "Vernon", "Monroe", "Juneau", "Adams"),]
wsi.sub <- wsi.sub[,-1]
wsi.mat <- as.matrix(wsi.sub)


cwd.nums.by.cnty.by.yr <- read.table("./data/cwd-numbers-by-county-by-year-sep.csv",
                      header=TRUE, sep=",")

cwd.nums.sep.sub <- subset(cwd.nums.by.cnty.by.yr, cwd.nums.by.cnty.by.yr$county %in% wi.cwd.area)
total.cwd.by.year <- data.frame(year=year.samp, nanlyzd=rep(0, length(year.samp)), cwd.pos=rep(0, length(year.samp)))

wi.indx <- order(wi.cwd.area)
ordrd.wi.cwd.area <- c()
for(i in 1:length(wi.indx)){
  ordrd.wi.cwd.area[i] <- wi.cwd.area[wi.indx[i]]
}
cwd.rate <- matrix(0, nrow=length(ordrd.wi.cwd.area), ncol=length(year.samp))
for(i in 1:nrow(cwd.nums.sep.sub)){
  for(j in 1:length(year.samp)){
    for(k in 1:length(ordrd.wi.cwd.area)){
      if(cwd.nums.sep.sub[i,]$year == year.samp[j] & cwd.nums.sep.sub[i,]$county == ordrd.wi.cwd.area[k]){
      cwd.rate[k,j] <- cwd.nums.sep.sub[i,]$npos/cwd.nums.sep.sub[i,]$nanalyzed
      }
    }
  }
}
```

```{r, make county bneighborhood matrix}
# arrange alphabetically
# this will work but let's figure out how to do this with the packages
wiCountyShp.studyarea <- subset(wiCountyShp, wiCountyShp$COUNTY_NAM %in% wi.cwd.area)
# subset to our study area
wiCountyShp.studyarea <- arrange(wiCountyShp.studyarea, wiCountyShp.studyarea$COUNTY_NAM)

W.nb <- poly2nb(wiCountyShp.studyarea)
# convert to a binary neighborhood matrix of 0's and 1's
adj.mat<-nb2mat(W.nb, style="B")
W <- nb2WB(W.nb)


# adj.mat<-matrix(0,nrow=length(wiCountyShp.studyarea$COUNTY_NAM),
#        ncol=length(wiCountyShp.studyarea$COUNTY_NAM))
# 
# for(i in 1:nrow(adj.mat)){
#     k = 1
#     while(k < W$num){
#       adj.mat[i, W$adj] <- 1  
#   }
# }

```

```{r, temporal wsi add var}
# so let's start by getting data for each year
# make a matrix for positives and number sampled
year.samp <- c(2002, 2003, 2004,2006,2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019,2020, 2021,2022)
# let's create our expected value matrix
dZIP <- nimbleFunction(
 run = function(x = integer(), lambda = double(),
                zeroProb = double(), log = logical(0, default = 0)) {
   returnType(double())
   ## First handle non-zero data
   if (x != 0) {
       ## return the log probability if log = TRUE
       if (log) return(dpois(x, lambda, log = TRUE) + log(1 - zeroProb))
       ## or the probability if log = FALSE
      else return((1 - zeroProb) * dpois(x, lambda, log = FALSE))
   }
   ## From here down we know x is 0
   totalProbZero <- zeroProb + (1 - zeroProb) * dpois(0, lambda, log = FALSE)
   if (log) return(log(totalProbZero))
   return(totalProbZero)
 })

registerDistributions(list(
    dZIP = list(
        BUGSdist = "dZIP(lambda, zeroProb)",
        discrete = TRUE,
        range = c(0, Inf),
        types = c('value = integer()', 'lambda = double()', 'zeroProb = double()')
     )))

modelcode <- nimbleCode({
    ### priors
      beta0~ dnorm(0.0, sd = 100)
      # vague prior WSI values for the county from the year prior
      beta1~ dnorm(0.0, sd = 1)
      ### likelihood
      gamma0~dnorm(0.0, sd=100)
      gamma1~dnorm(0.0, sd=1)

  # likelihood
  for (i in 1 : N) {
    for(j in 1 : years){  
        log(mu[i,j]) <- log(E[i,j]) + beta1*(S[i,j]) + beta0 + phi[i,j]
        logit(p[i,j]) <- gamma1*(S[i,j]) + gamma0 + psi[i,j]
        y[i, j] ~ dZIP(mu[i,j], p[i,j])
        }
  }
  # CAR model for spatial random effects
  # phi[] ~ dcar_normal(adj[1:L], weights[1:L], num[1:N], prec.c, zero_mean=0)
})

# change this to be a matrix of size i x j
p <- matrix(0, nrow=nrow(cwd.by.county.mat), ncol(cwd.by.county.mat))


## Specify data and initial values
wsi.sub <- wsi.mat[,-nrow(wsi.mat)]
wsi.sub <- wsi.sub[,-1]
wsi.sub <- wsi.sub[,-1]
constants <- list(N = nrow(cwd.by.county.mat), years=ncol(cwd.by.county.mat), S=wsi.sub)
data <- list(y = cwd.by.county.mat, p = p)
#gamma1=rep(0, nrow(cwd.by.county.mat))
inits <- list(beta0 =0,
              beta1 =0,
              gamma0=0,
              gamma1 = 0)
cwdspatmodel <- nimbleModel(modelcode, constants = constants, data = data, inits = inits)
c.cwdspatmodel <- compileNimble(cwdspatmodel)
confMC <- configureMCMC(cwdspatmodel, monitors = c('gamma0', 'gamma1','beta0', 'beta1'),  enableWAIC = TRUE)
cwdspatmcmc <- buildMCMC(confMC)
c.cwdspatmcmc <- compileNimble(cwdspatmcmc, project = cwdspatmodel)
## Run MCMC
mcmc.out <- runMCMC(c.cwdspatmcmc, niter=40000, nburnin=30000, thin=5, nchains=3, WAIC=TRUE)
## convert post samples as mcmc.list object and diagnose convergence using coda functions
post.samples <- mcmc.list(sapply(mcmc.out$samples,as.mcmc,simplify=FALSE))
pars <- c("beta0","beta1", 'gamma0', 'gamma1')
plot(post.samples, trace=TRUE, density=FALSE)
gelman.plot(post.samples)
autocorr.plot(post.samples)
## posterior summary
summary(post.samples)
## model assessment using WAIC value
mcmc.out$WAIC
```
```{r, plot our coefficients with the wsi vals}
betas <- c()
k <- 1
s<-summary(post.samples)
for(i in 1:nrow(cwd.by.county.mat)){
  for(j in 1:nrow(cwd.by.county.mat)){
    betas[k] <- s$statistics[paste0('beta1','[',i,", ",j,']', sep=""),'Mean']    
    k<-k+1
    }
}
# summary(post.samples)$statistics['beta1[1, 2]','Mean']
wsi.mat.c<-wsi.mat[,-ncol(wsi.mat)] # remove the 2021-2022 year
wsi.mat.c<-wsi.mat.c[,-1]
wsi.mat.c<-wsi.mat.c[,-1]
avg.wsi <- matrix(0, nrow=nrow(adj.mat), ncol=ncol(wsi.mat.c))
for(i in 1:nrow(adj.mat)){
  for(j in 1:ncol(wsi.mat.c)){
    print(sum(wsi.mat.c[,j]*adj.mat[i,])/sum(adj.mat[i,]))
    print(avg.wsi[i,j])
    avg.wsi[i,j] <- sum(wsi.mat.c[,j] * adj.mat[i,])/sum(adj.mat[i,])
  }
}
wsi.v <- as.vector(avg.wsi)

hist(betas)
```


```{r, prep data for plot}

summary(post.samples)[1]$statistics['beta0[2]','Mean']
regression.vals <- c()
for(i in 1:nrow(cwd.by.county.mat)){
  regression.vals[i]<-summary(post.samples)[1]$statistics[paste0('beta1','[', i, ']'),'Mean']
}


wiCountyShp.studyarea$reg.vals <- regression.vals
# 
# plot(wiCountyShp.studyarea)
# 
```
```{r, plot}
brks = c(-0.15, -0.1, -0.05, 0.01, 0, 0.01, 0.05)
color.pallete = rev(brewer.pal(6,"RdBu"))
class.raw = classIntervals(var=wiCountyShp.studyarea$reg.vals, n=6, style="fixed", fixedBreaks=brks, dataPrecision=5)
color.code.raw = findColours(class.raw, color.pallete)
class.cols = classIntervals(var=wiCountyShp.studyarea$reg.vals, n=6, style="fixed", fixedBreaks=brks, dataPrecision=5)
leg.txt = c("-0.15 <= -0.1", "-0.1 <= -.05", "-.05 <= -.01", "-.01 <= 0","0 >= 0.01", "0.01 >= 0.05")

plot(wiCountyShp.studyarea$geometry, col=color.code.raw)
title("CWD Totals As a Function of WSI Indicies" )
legend('bottomleft', legend=leg.txt, bty="n", horiz = FALSE, fill = color.pallete)
```


```{r, make scatterplot}

cwd.v<-as.vector(cwd.by.county.mat)
wsi.mat.c<-wsi.mat[,-ncol(wsi.mat)] # remove the 2021-2022 year
wsi.mat.c<-wsi.mat.c[,-1]
wsi.mat.c<-wsi.mat.c[,-1]
avg.wsi <- matrix(0, nrow=nrow(adj.mat), ncol=ncol(wsi.mat.c))
for(i in 1:nrow(adj.mat)){
  for(j in 1:ncol(wsi.mat.c)){
    print(sum(wsi.mat.c[,j]*adj.mat[i,])/sum(adj.mat[i,]))
    print(avg.wsi[i,j])
    avg.wsi[i,j] <- sum(wsi.mat.c[,j] * adj.mat[i,])/sum(adj.mat[i,])
  }
}

wsi.v <- as.vector(avg.wsi)
for(i in 1:length(cwd.v)){
  if(is.na(cwd.v[i])){
    cwd.v[i] <- 0
  }
}
bad.indx <-c()
k<-1
for(i in 1:length(cwd.v)){
  if(cwd.v[i] == 0){
    bad.indx[k] <- i
    k <- k + 1
  }
}
cwd.v <- cwd.v[-c(bad.indx)]
wsi.v <- wsi.v[-c(bad.indx)]
plot(cwd.v~wsi.v, ylab=" ", xlab=" ")
  abline(lm(cwd.v~wsi.v), col="red")
  title(main="County CWD Numbers vs Neighboring Counties WSI Values SW WI", xlab="WSI Values (2001-2021)", ylab="CWD Numbers")
  

```


```{r nimble CAR}
library(nimble)
library(spdep)
library(coda)
library(shapefiles)
library(sp)
library(rgdal)
## convert polygons to neighborhood object and 
## output spatial neighbors with weights needed for BUGS CAR 
W.nb <- poly2nb(studyarea)
W <- nb2WB(W.nb)
names(W)
# make township cwd.expectation
for(i in 1:nrow()){
  
}
## Specify the hierarchical model
modelcode <- nimbleCode({
    # likelihood
    for (i in 1 : N) {
        y[i] ~ dpois(mu[i])
        log(mu[i]) <- log(E[i]) + beta0 + beta1*aff[i] + phi[i] + theta[i]
        theta[i] ~ dnorm(0.0, prec.h)
        eta[i] <- theta[i] + phi[i]
    }
    # CAR model for spatial random effects
    phi[1:N] ~ dcar_normal(adj[1:L], weights[1:L], num[1:N], prec.c, zero_mean=0)
    ## priors
    beta0 ~ dnorm(0.0, sd = 100)  # vague prior on grand intercept
    beta1 ~ dnorm(0.0, sd = 100)  # vague prior on covariate effect
    prec.h ~ dgamma(0.001, 0.001)  
    prec.c ~ dgamma(0.1, 0.1)  
    # other prior choices for spatial/nonspatial precision
    # prec.h ~ dgamma(0.32761, 0.181)
    # prec.c ~ dgamma(0.1, 0.1)
    # prec.h ~ dgamma(3.2761,1.81)
    # prec.c ~ dgamma(1.0,1.0)
    sigma2 <- 1/prec.h
    tau2 <- 1/prec.c
    ## calculate alpha
    sd.h <- sd(theta[1:N]) # marginal SD of heterogeneity effects
    sd.c <- sd(phi[1:N])   # marginal SD of clustering (spatial) effects
    alpha <- sd.c/(sd.h + sd.c)
})
## Specify data and initial values
constants <- list(N = 56, L = length(W$adj), adj=W$adj, weights=W$weights, num=W$num, E = data.spPoly$expected, aff = data.spPoly$pcaff)
data <- list(y = data.spPoly$observed)
inits <- list(beta0 = 0, beta1 = 0, prec.h = 1, prec.c = 1)
## Build/Compile model, including steps: 
## (1) build model (2) compile model in C++ 
## (3) specify MCMC parameters to collect and create MCMC algorithm
LipModel <- nimbleModel(modelcode, constants = constants, data = data, inits = inits)
cLipModel <- compileNimble(LipModel)
confMC <- configureMCMC(LipModel, monitors = c('beta0','beta1','sigma2','tau2','eta','alpha'),  enableWAIC = TRUE)
LipMCMC <- buildMCMC(confMC)
cLipMCMC <- compileNimble(LipMCMC, project = cLipModel)
## Run MCMC
MCMC.out <- runMCMC(cLipMCMC, niter=60000, nburnin=10000, thin=5, nchains=3, WAIC=TRUE)
## convert post samples as mcmc.list object and diagnose convergence using coda functions
post.samples <- mcmc.list(sapply(MCMC.out$samples,as.mcmc,simplify=FALSE))
pars <- c("alpha","beta1","tau2","sigma2","eta[1]","eta[56]")
plot(post.samples[,pars], trace=TRUE, density=FALSE)
gelman.plot(post.samples[,pars])
autocorr.plot(post.samples[,pars])
## posterior summary
summary(post.samples[,pars])
## model assessment using WAIC value
MCMC.out$WAIC
```























