---
title: "prism-work"
author: "Alex Jack"
date: "2023-08-19"
output: html_document
---
```{r}
library("rgdal")
library("raster")
library("sf")
library("leafsync")
library("dplyr")
library("ggplot2")
library("stars")
library(spatialEco)
library(spatstat.random)
library(data.table)
library(geoR)
library(RColorBrewer)
library(spdep)
library(spatialreg)
library(classInt)
library(rgeos)
library(landscapemetrics)
library(nimble)
library(coda)
library(prism)
library(terra)
library(maps)
library(raster)

```
```{r}

wiShp<-st_read("./data/shapefiles/Wisconsin_State_Boundary_24K/Wisconsin_State_Boundary_24K.shp")
wiTwnshpShp <- st_read("./data/shapefiles/PLSS_Townships/PLSS_Townships.shp")
# use this for setting the directory for prism
prism_set_dl_dir("C:\\Users\\jackx\\Desktop\\prism-dat-f", create=FALSE)

# get_prism_dailys(type="ppt",
#                  dates="2012-04-03",
#                 keepZip = TRUE)
# get_prism_dailys(type="tmax",
#                  minDate="2000-11-01",
#                  maxDate="2022-05-31",
#                  keepZip = TRUE)
# get_prism_dailys(type="tmin",
#                  minDate="2000-11-01",
#                  maxDate="2022-05-31",
#                  keepZip = TRUE)
#prism_get_dl_dir(path="C:\\Users\\jackx022\\Desktop\\prism-dat-f")
wiShp<-st_read("./data/shapefiles/PLSS_Townships/PLSS_Townships.shp")
wiCountyShp <- st_read("./data/shapefiles/County_Boundaries_24K/County_Boundaries_24K.shp")

wiLC<-rast("./data/raster/wiscland2/wiscland2_dataset/level4/wiscland2_level4.tif")

# read in data
cwd.dat.pos<-read.table(
  "./data/wi-dat-cwd-pos.csv",
  sep=",", header=TRUE)
cwd.dat.analyzed<-read.table(
  "./data/cwd-dat-num-analyzed.csv",
  sep=",", header=TRUE)

# plot(wiLC)
```

```{r, calculate WSI matrix}
months <- c(11, 12, 1, 2, 3, 4, 5)

# constants for calculating WSI
tminThrshld <- -17.7 # number used by MNDNR
tmaxThrshld <- 0
pptThrshld <- 38 # 38 cm cited
# can set our month condition here for different measures of WSI
minDay <- '11-01' # november 1
maxDay <- '05-10' # may 10
# these are for the WSI index; for year n-1

# NOTE start testing at 2017
years<-c(seq(2018,2022,1))
# years.s<-c(seq(2001, 2002, 1))
# DON'T DELETE; only for getting 
pdStackRaw<-pd_stack(prism_archive_subset("ppt",
                                          "monthly",
                                          years=2001))
# now we need to reproject these into the same CRS
wiShp.e<-projectExtent(wiCountyShp, pdStackRaw)
# great now we have pdStackWI
# now we can crop the pdStack data into our study extent
pdStackWI<-terra::crop(pdStackRaw, wiShp.e)
# first let's get these into the same CRS
wiTwnShpSpat<-as(wiTwnshpShp, "Spatial")

# get the spat data into the same CRS
wiTwnShpSpat<-spTransform(wiTwnShpSpat, crs(wiCountyShp))
cumPPT <- 0
wsi.mat <- matrix(0, nrow = nrow(wiTwnshpShp), ncol=length(years))
k<-1
for(i in 1:length(years)){
  minDate <- paste0(years[i], "-", minDay)
  maxDate <- paste0(years[i] + 1, "-", maxDay)
  #if(exists)
  
  # note can use prism_archive_verify
  # or prism_archive_clean
  # maybe need to change this to iterate over each day
  pptRaw<-pd_stack(prism_archive_subset("ppt",
                                        "daily",
                                         minDate=minDate,
                                         maxDate=maxDate))
  tmaxRaw<-pd_stack(prism_archive_subset("tmax",
                                         "daily",
                                         minDate=minDate,
                                         maxDate=maxDate))
  tminRaw<-pd_stack(prism_archive_subset("tmin",
                                         "daily",
                                         minDate=minDate,
                                         maxDate=maxDate))
  pptWI <- terra::crop(pptRaw, wiShp.e)
  tmaxWI<- terra::crop(tmaxRaw, wiShp.e)
  tminWI <- terra::crop(tminRaw, wiShp.e)
  ppt_by_twnshp <- terra::extract(pptWI,
                                wiTwnShpSpat,
                                method="bilinear")
  tmax_by_twnshp <- terra::extract(tmaxWI,
                                wiTwnShpSpat,
                                method="bilinear")
  tmin_by_twnshp <- terra::extract(tminWI,
                                wiTwnShpSpat,
                                method="bilinear")
    # index for the current day
    k <- 1
    
    for(j in 1:length(ppt_by_twnshp)){
      for(k in 1:length(ppt_by_twnshp[[j]][1,])){
        # need to check if any of the values are NA
        pptVals<-as.vector(ppt_by_twnshp[[j]][,k])
        tminVals<-as.vector(tmin_by_twnshp[[j]][,k])
        tmaxVals<-as.vector(tmax_by_twnshp[[j]][,k])
        if(!(any(is.na(pptVals), is.na(tminVals), is.na(tmaxVals)))){
          # note that PRISM data is reported in MM; need CM divide by 10
          avgPPT <- mean(pptVals) / 10 # average ppt for a given township for day k
          avgTMin <- mean(tminVals) # average min. temp for a given township for day k 
          avgTMax <- mean(tmaxVals)
          avgTmp <- mean(c(tminVals, tmaxVals))
      if(avgPPT > 0 & avgTmp <= 0){
        cumPPT = cumPPT + avgPPT
      }
      # snow on the ground but above freezing
        
      # TODO: this is where factors such as latitude can be taken into account
      # could also take into account landscape factors here as well
      # note that the measurements are in mm for precip and C for temp
      # https://prism.oregonstate.edu/FAQ/#:~:text=What%20units%20are%20the%20data,want%20maps%20in%20those%20units
      if(cumPPT > 0 & avgTmp > 0){
        # from Dawe and Boutin 2012 Journal of Wildlife Research
        meltFactor <- (1.88 + 0.007 * avgPPT) * (1.8 * avgTmp) + 1.27
        cumPPT = cumPPT - meltFactor # some melt factor update later with lit found
      }
      if((cumPPT < pptThrshld & avgTMin > tminThrshld) || 
         (cumPPT > pptThrshld & avgTMin < tminThrshld)){
            wsi.mat[j,i] <- wsi.mat[j,i] + 1
        }
      if(cumPPT > pptThrshld & avgTMin < tminThrshld){
            wsi.mat[j,i] <- wsi.mat[j,i] + 2
      }
      # in this case all  of the days in 
      # a particular township have been processed
          }
      }
      cumPPT <- 0
      }
}
```
```{r}
wi.small.subset <- c("Dane", "Columbia", "Dodge", "Jefferson", "Calumet", "Chippewa", "Crawford", "Eau Claire", "Iowa", "Richland", "Adams", "Walworth")
wiCountyStudy<-subset(wiCountyShp, COUNTY_NAM %in% wi.small.subset)
wiTwnshpShp<-st_transform(wiTwnshpShp, st_crs(wiCountyStudy))
wiTwnShpStudy <- st_crop(wiTwnshpShp, extent(wiCountyStudy))
# wiStudyShp will represent the interesection between our township and county data
wiStudyShp <- st_intersection(wiTwnshpShp, wiCountyStudy)
# now we can add the uid back instudyarea$uid <- 0
wiStudyShp$uid <- 0
for(j in 1:nrow(wiStudyShp)){
    wiStudyShp[j,]$uid <- paste0(wiStudyShp[j,]$COUNTY_FIP, "-",
                             wiStudyShp[j,]$TWP, "-",
                             wiStudyShp[j,]$RNG, "-",
                             wiStudyShp[j,]$DIR_ALPHA)

}
# now we can match up our cwd data and township data
wiStudyShp <- wiStudyShp[-c(which(!c(wiStudyShp$uid) %in% c(cwd.dat.pos$uid))),]
# let's do the same with our cwd data
cwd.dat.analyzed$fid <- 0
cwd.dat.pos$fid <- 0
for(i in 1:nrow(cwd.dat.pos)){
  for(j in 1:nrow(wiStudyShp)){
    if(wiStudyShp[j,]$uid == cwd.dat.pos[i,]$uid){
       cwd.dat.pos[i,]$fid <- wiStudyShp[j,]$FID
        cwd.dat.analyzed[i,]$fid <- wiStudyShp[j,]$FID 
    }
  }
}
# now we can arrange, make sure we're referring to the same shape
cwd.dat.pos<-cwd.dat.pos[-c(which(cwd.dat.pos$fid == 0)),] # get rid of those without matching spatial fid
cwd.dat.analyzed<-cwd.dat.analyzed[-c(which(cwd.dat.analyzed$fid==0)),]
wiStudyShp<-arrange(wiStudyShp, wiStudyShp$uid)
cwd.dat.pos<-arrange(cwd.dat.pos, cwd.dat.pos$uid)
cwd.dat.analyzed<-arrange(cwd.dat.analyzed, cwd.dat.analyzed$uid)

# now let's make matrices out of these
# tmp.df <- data.frame()
# tmp.df<-cbind(wiStudyShp$X2001.ppt.mean,
#               wiStudyShp$X2002.ppt.mean,
#               wiStudyShp$X2003.ppt.mean,
#               wiStudyShp$X2004.ppt.mean,
#               wiStudyShp$X2005.ppt.mean,
#               wiStudyShp$X2006.ppt.mean,
#               wiStudyShp$X2007.ppt.mean,
#               wiStudyShp$X2008.ppt.mean,
#               wiStudyShp$X2009.ppt.mean,
#               wiStudyShp$X2010.ppt.mean,
#               wiStudyShp$X2011.ppt.mean,
#               wiStudyShp$X2012.ppt.mean,
#               wiStudyShp$X2013.ppt.mean,
#               wiStudyShp$X2014.ppt.mean,
#               wiStudyShp$X2015.ppt.mean,
#               wiStudyShp$X2016.ppt.mean,
#               wiStudyShp$X2017.ppt.mean,
#               wiStudyShp$X2018.ppt.mean,
#               wiStudyShp$X2019.ppt.mean,
#               wiStudyShp$X2020.ppt.mean,
#               wiStudyShp$X2021.ppt.mean)
# for(i in 1:nrow(cwd.dat.pos)){
#   for(j in 1:ncol(cwd.dat.pos)){
#     if(is.na(cwd.dat.pos[i,j])){
#       cwd.dat.pos[i,j] <-0
#     }
#     if(is.na(cwd.dat.analyzed[i,j])){
#       cwd.dat.analyzed[i,j] <-0
#     }
#   }
# }
# 
# cwdMatPos <- as.matrix(cwd.dat.pos[,-c(1, ncol(cwd.dat.pos))])
# cwdMatAnalyzed <- as.matrix(cwd.dat.analyzed[,-c(1, ncol(cwd.dat.analyzed))])
# cwd.mat.anlyzd <- cwd.mat.anlyzd[,-ncol(cwd.mat.anlyzd)]
# now let's get these set up for temporal analysis
```
```{r save}
write.table(cwdMatPos, file="cwd.pos")
write.table(cwdMatAnalyzed, file="cwd.analyzed")
write.table(tmp.df, file="prism.ppt.study.area")
cwdMatPos<-read.table("cwd.pos")
cwdMatAnalyzed <- read.table("cwd.analyzed")
prismPPTStudy <- read.table("prism.ppt.study.area")
```

```{r, create WSI}

# we can count the number of significant snowfalls, perhaps find some
# literature on deer migration relative to the amount of snow that falls

```

```{r, bayesian analysis}

# year samp will be the number of years we're getting CWD samples for
yearSamp<-c(seq(2002,2022, 1))
# let's create our expected value
cwd.r <- rep(0, ncol(cwdMatPos))
for(j in 1:ncol(cwdMatPos)){
  cwd.r[j]<-sum(cwdMatPos[,j])/sum(cwdMatAnalyzed[,j])  
}

# create neighborhood matrix
W.nb <- poly2nb(wiStudyShp)
W <- nb2WB(W.nb)
# TODO: AVERAGE over years for WSI
dZIP <- nimbleFunction(
 run = function(x = integer(), lambda = double(),
                zeroProb = double(), log = logical(0, default = 0)) {
   returnType(double())
   ## First handle non-zero data
   if (x != 0) {
       ## return the log probability if log = TRUE
       if (log) return(dpois(x, lambda, log = TRUE) + log(1 - zeroProb))
       ## or the probability if log = FALSE
      else return((1 - zeroProb) * dpois(x, lambda, log = FALSE))
   }
   ## From here down we know x is 0
   totalProbZero <- zeroProb + (1 - zeroProb) * dpois(0, lambda, log = FALSE)
   if (log) return(log(totalProbZero))
   return(totalProbZero)
 })

registerDistributions(list(
    dZIP = list(
        BUGSdist = "dZIP(lambda, zeroProb)",
        discrete = TRUE,
        range = c(0, Inf),
        types = c('value = integer()', 'lambda = double()', 'zeroProb = double()')
     )))

modelcode <- nimbleCode({
    ## priors
    beta0 ~ dnorm(0.0, sd = 5)  # vague prior on intercept
    beta1~ dnorm(0.0, sd = 1)
    gamma0 ~ dnorm(0.0, sd=5)  
    gamma1 ~ dnorm(0.0, sd=1)
    prec.c ~ dgamma(0.1, 0.1)
    prec.ci ~ dgamma(0.1, 0.1)
    prec.h ~ dgamma(0.1, 0.1)
    prec.hi ~ dgamma(0.1, 0.1)
        # CAR model for spatial random effects
    phi[1:n.twnshps] ~ dcar_normal(adj[1:L], weights[1:L], num[1:n.twnshps], prec.c, zero_mean=0)
    psi[1:n.twnshps] ~ dcar_normal(adj[1:L], weights[1:L], num[1:n.twnshps], prec.ci, zero_mean=0)
  # likelihood
  for (i in 1 : n.twnshps) {
    for(j in 1: n.years){
        log(mu[i,j]) <-  s[i,j]*mu.hat[j] + beta1*precip[i,j] + beta0 + phi[i] + theta[i]
        logit(p[i,j]) <- s[i,j]*mu.hat[j] +  gamma1*precip[i,j] + gamma0 + psi[i] + eta[i]
        theta[i] ~ dnorm(0.0, prec.c)
        eta[i] ~ dnorm(0.0, prec.ci)
        y[i,j] ~ dZIP(mu[i,j], p[i,j])
        epsilon[i] <- theta[i] + phi[i]
        epsilon.hat[i] <- psi[i] + eta[i]
        }
  }
        ## calculate alpha
    sd.h <- sd(phi[1:n.twnshps]) # marginal SD of heterogeneity effects
    sd.c <- sd(theta[1:n.twnshps])   # marginal SD of clustering (spatial) effects
    sd.hi <- sd(psi[1:n.twnshps])
    sd.ci <- sd(eta[1:n.twnshps])
    alpha = sd.h / (sd.c + sd.h)
    alpha.i <- sd.hi/(sd.ci + sd.hi) 
    sigma2 <- 1/prec.h
    tau2 <- 1/prec.c
    sigma2i <- 1/prec.hi
    tau2i <- 1/prec.ci
})

p <- matrix(0, nrow=nrow(cwdMatAnalyzed), ncol=ncol(cwdMatAnalyzed))
## Specify data and initial values
constants <- list(n.twnshps = nrow(cwdMatAnalyzed),
                  mu.hat=cwd.r,
                  n.years=length(yearSamp),
                  L = length(W$adj),
                  adj=W$adj,
                  weights=W$weights,
                  num=W$num)
data <- list(y = cwdMatPos, s = cwdMatAnalyzed, p = p, precip=wiPPTStudy)
inits <- list(beta0 = 0, beta1=0, gamma1=0, gamma0 = 0)


## Build/Compile model, including steps:
## (1) build model (2) compile model in C++
## (3) specify MCMC parameters to collect and create MCMC algorithm
cwdspatmodel <- nimbleModel(modelcode, constants = constants, data = data, inits = inits)
c.cwdspatmodel <- compileNimble(cwdspatmodel)

confMC <- configureMCMC(cwdspatmodel, monitors = c('beta0', 'beta1', 'gamma1','gamma0', 'alpha', 'alpha.i', 'tau2', 'sigma2', 'tau2i', 'sigma2i'),  enableWAIC = TRUE)
cwdspatmcmc <- buildMCMC(confMC)
c.cwdspatmcmc <- compileNimble(cwdspatmcmc, project = cwdspatmodel)
## Run MCMC
mcmc.out <- runMCMC(c.cwdspatmcmc,
                    niter=40000,
                    nburnin=30000,
                    thin=5,
                    nchains=3,
                    WAIC=TRUE)
## convert post samples as mcmc.list object and diagnose convergence using coda functions
post.samples <- mcmc.list(sapply(mcmc.out$samples,
                                 as.mcmc,simplify=FALSE))
pars <- c("beta0",'gamma0', 'beta1', 'gamma1', 'sd.h', 'sd.hi', 'epsilon', 'epsilon.hat')
plot(post.samples, trace=TRUE, density=FALSE)
gelman.plot(post.samples)
autocorr.plot(post.samples)
## posterior summary
summary(post.samples)
## model assessment using WAIC value
mcmc.out$WAIC
```
